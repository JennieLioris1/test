{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282763c9-7ee9-4aa8-975f-f939ab26c041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 14:37:22.227518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import category_encoders as ce\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145496d0-a0b3-4eb2-8333-e2301a5cbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATAFRAMES PER ORIGIN\n",
    "\n",
    "#function creating a dictionary of dataframes \n",
    "#from an initial dataframe.\n",
    "#Each dataframe corresponds to a specific value \n",
    "#of variable ORIGIN\n",
    "#The dataframe is created only if it has \n",
    "#at least a desired number of entries \n",
    "#since there may be \"origins\" associated with too few entries\n",
    "\n",
    "#it returns di_dataframes_per_origin where\n",
    "#key=name of orogin airport\n",
    "#value = the related df and sorted according \n",
    "#'FL_DATE', 'DEP_TIME'\n",
    "\n",
    "def fct_create_di_dataframes_per_origin(\n",
    "    val_dataframe,\n",
    "    val_name_col_origin=\"ORIGIN\",\n",
    "    val_name_col_1_sort='FL_DATE',\n",
    "    val_name_col_2_sort='DEP_TIME',\n",
    "    val_admissible_min_nb_observatios=80):\n",
    "\n",
    "    #key=name of orogin airport\n",
    "    #value = the related df\n",
    "    di_dataframes_per_origin={}\n",
    "    \n",
    "    li_origins=val_dataframe[val_name_col_origin].unique().tolist()\n",
    "    \n",
    "    for i in li_origins:\n",
    "        df=val_dataframe[val_dataframe[val_name_col_origin]==i]\n",
    "        \n",
    "        #if the dataframe has at least val_admissible_min_nb_observatios entries\n",
    "        #we create it otherwise we ignore it\n",
    "        \n",
    "        if df.shape[0]>=val_admissible_min_nb_observatios:\n",
    "            \n",
    "            df=df.sort_values([val_name_col_1_sort, val_name_col_2_sort])\n",
    "            \n",
    "            di_dataframes_per_origin[i]=df\n",
    "     \n",
    "    #print(\"in fct len(li_dataframes_per_origin)\",len(li_dataframes_per_origin))\n",
    "    return di_dataframes_per_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f17e2d8-00bd-4024-9fb7-95e2d9b1d9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IAD',\n",
       " 'SEA',\n",
       " 'LAX',\n",
       " 'ORD',\n",
       " 'TPA',\n",
       " 'DCA',\n",
       " 'LGA',\n",
       " 'OAK',\n",
       " 'BDL',\n",
       " 'PDX',\n",
       " 'BIL',\n",
       " 'BOS',\n",
       " 'PHL',\n",
       " 'LAS',\n",
       " 'DTW',\n",
       " 'DEN',\n",
       " 'OMA',\n",
       " 'RDU',\n",
       " 'SFO',\n",
       " 'ONT',\n",
       " 'SMF',\n",
       " 'MSP',\n",
       " 'GEG',\n",
       " 'PHX',\n",
       " 'MCO',\n",
       " 'MSY',\n",
       " 'RNO',\n",
       " 'JFK',\n",
       " 'BUF',\n",
       " 'DSM',\n",
       " 'SAN',\n",
       " 'MCI',\n",
       " 'MDT',\n",
       " 'CLE',\n",
       " 'OKC',\n",
       " 'PVD',\n",
       " 'FSD',\n",
       " 'IAH',\n",
       " 'EWR',\n",
       " 'BWI',\n",
       " 'BTV',\n",
       " 'ROC',\n",
       " 'ICT',\n",
       " 'MIA',\n",
       " 'RIC',\n",
       " 'TUL',\n",
       " 'HNL',\n",
       " 'DFW',\n",
       " 'JAC',\n",
       " 'PIT',\n",
       " 'AUS',\n",
       " 'SNA',\n",
       " 'SJC',\n",
       " 'BOI',\n",
       " 'ALB',\n",
       " 'DAY',\n",
       " 'GRR',\n",
       " 'CMH',\n",
       " 'HDN',\n",
       " 'SLC',\n",
       " 'SJU',\n",
       " 'EGE',\n",
       " 'ABQ',\n",
       " 'SAT',\n",
       " 'ATL',\n",
       " 'CLT',\n",
       " 'RSW',\n",
       " 'MHT',\n",
       " 'KOA',\n",
       " 'OGG',\n",
       " 'LIH',\n",
       " 'PSP',\n",
       " 'STL',\n",
       " 'TUS',\n",
       " 'STT',\n",
       " 'IND',\n",
       " 'JAX',\n",
       " 'CVG',\n",
       " 'BZN',\n",
       " 'RAP']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#read DataFrame from pickle file\n",
    "df_2009_1= pd.read_pickle(\"my_df_2009_1.pkl\")\n",
    "\n",
    "di=fct_create_di_dataframes_per_origin(\n",
    "val_dataframe=df_2009_1)\n",
    "\n",
    "list(di.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda73a99-bb3d-4835-9d3d-1ac1c799cc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf634e5-3917-4bfb-9d1d-42df21f5a36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84bd47c-aec8-4bd6-8039-dd0b52786992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FUNCTION  creating the list targets for a single dataframe \n",
    "\n",
    "def fct_create_li_target_single_df(\\\n",
    "val_dataframe,val_target_column_name=\"ARR_DELAY\"):\n",
    "    \n",
    "    #li_targets=[] the ith element is  targets for the ith entry\n",
    "    \n",
    "    \n",
    "    li_targets=val_dataframe[val_target_column_name].values\n",
    "    \n",
    "    #print(\"li_targets\",li_targets)\n",
    "    #import sys\n",
    "    #sys.exit()\n",
    "        \n",
    "        \n",
    "    return li_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b274c7be-0e7c-455f-8d50-351c8d716448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VECTORIZATION  -  CATEGORICAL VARIABLES\n",
    "\n",
    "#fct which vectorizes object type columns of a single dataframe\n",
    "#IT RETURNS A VECTORIZED DATAFRAME\n",
    "#the encoder employed is \n",
    "#LeaveOneOutEncoder which is a traget based encoder\n",
    "#the advantages of this encoder is that \n",
    "#it doesn't increases the number of variables\n",
    "#as it doesn't create additional variables\n",
    "\n",
    "#this function vectorizes the  dataframe \n",
    "#which is passed as value to the argument val_dataframe\n",
    "\n",
    "def fct_vectorization_obj_cols_single_df(\n",
    "    val_dataframe,\n",
    "    val_li_name_cols_to_ignore=['FL_DATE','ORIGIN'],\n",
    "    val_li_name_cols_to_copy=['DEST']):\n",
    "    \n",
    "\n",
    "    \n",
    "    #print(\"li_obj_cols\",li_obj_cols)\n",
    "\n",
    "    #***************\n",
    "    #FOR VERIFICATION PURPOSES WE KEEP AS OBJECT TYPE A COPIE OF VARIABLES, \n",
    "    #OP_CARRIER\n",
    "    #ORIGIN\n",
    "    #DEST\n",
    "\n",
    "    #df_2009_1['OP_CARRIER_1']=df_2009_1['OP_CARRIER']\n",
    "    #df_2009_1['ORIGIN_1']=df_2009_1['ORIGIN']\n",
    "    #***************\n",
    "    \n",
    "    #FOR VERIFICATION PURPOSES WE KEEP AS OBJECT TYPE A COPIE OF VARIABLE DEST\n",
    "    # for the model 1 where we have a separate dataframe per ORIGIN and we employ\n",
    "    #a single OPERATOR that is UA we do not vectorize variables as we will not use them.\n",
    "    #But if we need to study many ORIGINS (model 2) and/or many OPERATORS  simultaneously \n",
    "    #that is airline companies then we would make a copy of these variables \n",
    "    #for verification reasons\n",
    "    \n",
    "    #the list with the column names to  ignire during the vectorization\n",
    "    val_li_cols_to_ignore=val_li_name_cols_to_ignore\n",
    "    \n",
    "    #print(\"val_li_name_cols_to_copy\",val_li_name_cols_to_copy)\n",
    "    \n",
    "    for i in val_li_name_cols_to_copy:\n",
    "        \n",
    "        #print(\"IN fct_vectorization_obj_cols_single_df, i\",i)\n",
    "        \n",
    "        val_dataframe[i+str('_1')]=val_dataframe[i]\n",
    "    \n",
    "        val_li_cols_to_ignore.append(i+str('_1'))\n",
    "                                                 \n",
    "    #the list with the object type columns to vectorize                                            \n",
    "    li_obj_cols=[i for i in val_dataframe.select_dtypes(exclude=np.number).columns.tolist() \n",
    "             if i not in val_li_cols_to_ignore]\n",
    "                                                 \n",
    "    encoder = ce.LeaveOneOutEncoder(return_df=True)\n",
    "\n",
    "    val_dataframe[li_obj_cols]=encoder.fit_transform(val_dataframe[li_obj_cols],val_dataframe['ARR_DELAY'])\n",
    "\n",
    "    \n",
    "    return val_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d505b3-0113-409f-ba00-64cf50172a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fct_measure_mean_observations_per_day_single_df(\n",
    "    val_name_df,\n",
    "    val_name_column_date='FL_DATE'):\n",
    "    \n",
    "    res = (pd.to_datetime(val_name_df[val_name_column_date])\n",
    "       .dt.floor('d')\n",
    "       .value_counts()\n",
    "       .rename_axis(val_name_column_date)\n",
    "       .reset_index(name='count'))\n",
    "    \n",
    "    res1=res.sort_values(by=[val_name_column_date])\n",
    "        \n",
    "    mean_nb_obs=round(res1[\"count\"].mean())\n",
    "        \n",
    "    return mean_nb_obs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4690e2ec-74de-480e-a354-3e2245e85201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function standardizing training samples of single dataframe \n",
    "#it returns \n",
    "#[standardized data array, number_train_samples,\n",
    "#number_val_samples,number_test_samples,mean,std] \n",
    "\n",
    "def fct_stand_single_df(\n",
    "    val_name_df,\n",
    "    val_proportion_train_set=0.5,\n",
    "    val_proportion_val_set=0.25):\n",
    "    \n",
    "    \n",
    "    #we exctract the numerical columns\n",
    "    li_num_cols=val_name_df.select_dtypes(include=np.number).columns.tolist()\n",
    "    #print(li_num_cols)\n",
    "    #break\n",
    "    \n",
    "    #we keep the target variable in the data but we do not keep the date as it is\n",
    "    #object type variable and we only keep numerical ones\n",
    "\n",
    "    raw_data=val_name_df[li_num_cols].to_numpy()\n",
    "    #raw_data.shape\n",
    "    \n",
    "    #Computing the number of samples we'll use for each data split\n",
    "\n",
    "    #samples we’ll use for each data split\n",
    "\n",
    "    #50% of the data for training, \n",
    "    #the following 25% for validation, and \n",
    "    #the last 25% for testing.\n",
    "    \n",
    "    num_train_samples = int(val_proportion_train_set * len(raw_data))\n",
    "\n",
    "    num_val_samples = int(val_proportion_val_set * len(raw_data))\n",
    "\n",
    "    num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
    "    \n",
    "    \n",
    "    mean = raw_data[:num_train_samples].mean(axis=0)\n",
    "    raw_data -= mean\n",
    "    std = raw_data[:num_train_samples].std(axis=0)\n",
    "    #we replace all std equal to zero  by 0.0001\n",
    "    std[std  == 0] = 0.0001\n",
    "    raw_data /= std\n",
    "    \n",
    "    return[raw_data,num_train_samples,num_val_samples,num_test_samples,mean,std]\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d447cc07-7328-4452-bb86-c198f6bfe5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function examining the existence of missing (null) values for a list dataframe\n",
    "#t returns 0/1 if no/at least one  dataframe has missing  values\n",
    "#and also prints a message\n",
    "\n",
    "def fct_examine_existence_null_vals_dataframe(\n",
    "    val_li_dataframes):\n",
    "    \n",
    "    rep=0\n",
    "    \n",
    "    for i in val_li_dataframes:\n",
    "        \n",
    "        a=i.isnull().sum().sum()\n",
    "        \n",
    "        if a!=0:\n",
    "            print(\"Missing values for current dataframe\")\n",
    "            print(i.head())\n",
    "            rep=1\n",
    "            return rep\n",
    "        \n",
    "    print(\"No missing values in any dataframe of the considered list\")\n",
    "    print()\n",
    "    \n",
    "    return rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce203afc-1c01-44f2-a5ee-9d9f83b0aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fct_plot_correlation(\\\n",
    "    v_data,\\\n",
    "    v_folder_figure,\\\n",
    "    v_name_fig=\"fig_cor_vector_data_heatmap.png\"):\n",
    "    \n",
    "    plt.figure(figsize=(14,7))\n",
    "    #plt.figure(figsize=(20,15))\n",
    "\n",
    "    # Create a custom divergin palette\n",
    "    #cmap = sns.diverging_palette(100, 7, s=75, l=40,n=5, center=\"light\", as_cmap=True)\n",
    "    cmap = sns.diverging_palette(145, 300, s=60, as_cmap=True)\n",
    "    \n",
    "    #Create a mask\n",
    "    mask = np.triu(np.ones_like(v_data.corr(numeric_only = True), dtype=bool))\n",
    "    np.fill_diagonal(mask, False)\n",
    "\n",
    "    #scale all fonts in your legend and on the axes.\n",
    "    #sns.set(font_scale=1.4)\n",
    "    sns.set(font_scale=1)\n",
    "    #cmap=\"PiYG\n",
    "    heatmap = sns.heatmap(v_data.corr(numeric_only = True), vmin=-1, vmax=1, annot=True,\n",
    "    annot_kws={'fontsize': 8},\n",
    "    linewidths=0.5, linecolor='m',\n",
    "    cmap=cmap, mask=mask)\n",
    "    heatmap.set_title('Correlation coeffcient matrix',\n",
    "    fontdict={'fontsize':15}, color=\"darkmagenta\",weight='bold',pad=12)\n",
    "    plt.yticks(rotation=30,fontsize=8.5,fontweight=\"bold\",color=\"darkmagenta\")\n",
    "    plt.xticks(rotation=20,fontsize=8.3,fontweight=\"bold\",color=\"darkmagenta\")\n",
    "    plt.savefig(v_folder_figure+\"/\"+v_name_fig)\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1065fc-fc52-49d9-9df1-cac3ae6b6e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#OUTPUT OF THE FUNCTION\n",
    "\n",
    "#it returns \n",
    "\n",
    "#[rep_stand_arrays,\\\n",
    "#val_li_targets_df,\\\n",
    "#val_mean_observations_per_day,\n",
    "#mean,std]\n",
    "\n",
    "\n",
    "\n",
    "#rep_stand_arrays=[standardized data array, \n",
    "#number_train_samples ,\n",
    "#number_val_samples,list_number_test_samples,\n",
    "#mean,std]\n",
    "\n",
    "#standardized data array= arrays, \n",
    "\n",
    "#number_train_samples=  the number of training samples\n",
    "\n",
    "#number_val_samples=  the number of validation samples\n",
    "\n",
    "\n",
    "#number_test_samples= the number of test samples\n",
    "\n",
    "#Model 2= the model where we have a single dataframe with all the origins\n",
    "\n",
    "\n",
    "def fct_creation_data_arrays_model_2(\n",
    "    var_dataframe,\n",
    "    var_folder_figure,\n",
    "    var_name_col_origin=\"ORIGIN\",\n",
    "    var_li_name_col_to_copy=['DEST'],\n",
    "    var_name_column_date='FL_DATE',\n",
    "    var_name_col_2_sort='DEP_TIME',\n",
    "    var_name_target_variable=\"ARR_DELAY\",\n",
    "    var_li_cols_to_ignore=['FL_DATE','ORIGIN'],\n",
    "    var_proportion_train_set=0.5,\n",
    "    var_proportion_val_set=0.25,\n",
    "    var_name_fig=\"fig_cor_vector_data_heatmap.png\"\n",
    "):\n",
    "    \n",
    "    \n",
    "    #we examine null values in each dataframe\n",
    "    #val_existence_missing_values=O/1 no missing/missing values\n",
    "    val_existence_missing_values=\\\n",
    "    fct_examine_existence_null_vals_dataframe(\n",
    "        val_li_dataframes=[var_dataframe])\n",
    "    \n",
    "    #WE HAVE PREVIOUSLY CHECKED (OUTSIDE OF THIS FUNCTION)\n",
    "    #THAT THERE ARE NO MISSING VALUES\n",
    "    if val_existence_missing_values==1:\n",
    "        print(\"IN FCT fct_creation_data_arrays_model_2,\\\n",
    "        EXISTENCE MISSING VALUES :\",val_existence_missing_values==1)\n",
    "    \n",
    "    #we create the list with the targets for each dataframe\n",
    "    val_li_targets_df=fct_create_li_target_single_df(\\\n",
    "    val_dataframe=var_dataframe,\n",
    "    val_target_column_name=var_name_target_variable)\n",
    "    \n",
    "\n",
    "    \n",
    "    # VECTORIZATION  -  CATEGORICAL VARIABLES\n",
    "\n",
    "    #we vectorize all columns but date and origin\n",
    "    #we keep o copy of te destination column DEST\n",
    "    #as an object type variable with the name DEST_1\n",
    "    #we do it i cse we need to rapidly detect the destination by name\n",
    "\n",
    "    val_vectorized_df=fct_vectorization_obj_cols_single_df(\n",
    "    val_dataframe=var_dataframe,\n",
    "    val_li_name_cols_to_ignore=var_li_cols_to_ignore,\n",
    "    val_li_name_cols_to_copy=var_li_name_col_to_copy)\n",
    "    \n",
    "    \n",
    "\n",
    "    #print(\"Head Vectorized dataframe: \",\\\n",
    "    #val_vectorized_dfs[0].head()\n",
    "    #print()\n",
    "    \n",
    "    inform_vectorized_df=val_vectorized_df.info()\n",
    "    \n",
    "    print(\"information vectorized df:\",inform_vectorized_df)\n",
    "    print()\n",
    "    \n",
    "    #PLOT CORRELATION\n",
    "    fct_plot_correlation(\\\n",
    "    v_data=val_vectorized_df,\\\n",
    "    v_folder_figure=var_folder_figure,\\\n",
    "    v_name_fig=var_name_fig)\n",
    "    \n",
    "    \n",
    "    #DATA EXAMINATION\n",
    "    #we measure the mean number of observations  per day per dataframe\n",
    "\n",
    "    val_mean_observations_per_day=\\\n",
    "    fct_measure_mean_observations_per_day_single_df(\n",
    "    val_name_df=val_vectorized_df,\n",
    "    val_name_column_date=var_name_column_date)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"mean number of observations for the vectorized dataframe: \",\n",
    "          val_mean_observations_per_day)\n",
    "    print()\n",
    "    \n",
    "\n",
    "    \n",
    "    #Preparing the data\n",
    "\n",
    "    #Problem formulation\n",
    "    #given data covering the previous x(=1) days and sampled once per day per flight, \n",
    "    #can we predict the arrival delay in next x(=1) day\n",
    "\n",
    "\n",
    "    #Standardize-Normalize the data\n",
    "\n",
    "\n",
    "    \n",
    "    #There are two types of scaling of your data \n",
    "    #that you may want to consider: \n",
    "    #normalization and standardization.\n",
    "\n",
    "    #INPUT DATA\n",
    "\n",
    "    #Standardizing a dataset involves rescaling \n",
    "    #the distribution of values \n",
    "    #so that the mean of observed values is 0 and \n",
    "    #the standard deviation is 1. \n",
    "    #It is sometimes referred to as “whitening.”\n",
    "    #This can be thought of as subtracting the mean value \n",
    "    #or centering the data.\n",
    "\n",
    "\n",
    "    #Normalization is a rescaling of the data from the original \n",
    "    #range so that all values are within the range of 0 and 1.\n",
    "\n",
    "\n",
    "\n",
    "    #If the distribution of the quantity is normal, then \n",
    "    #it should be standardized, otherwise the data should be normalized. \n",
    "    #This applies if the range of quantity values is large \n",
    "    #(10s, 100s, etc.) or \n",
    "    #small (0.01, 0.0001).\n",
    "\n",
    "    #f the quantity values are small (near 0-1) and \n",
    "    #the distribution is limited (e.g. standard deviation near 1) \n",
    "    #then perhaps you can get away with no scaling of the data.\n",
    "\n",
    "\n",
    "    #If in doubt, normalize the input sequence. \n",
    "    #If you have the resources, explore modeling with the raw data, \n",
    "    #standardized data, and normalized data and see \n",
    "    #if there is a beneficial difference in the performance \n",
    "    #of the resulting model.\n",
    "\n",
    "\n",
    "\n",
    "    # We’re going to use the first num_train_samples timesteps \n",
    "    #as training data, so we’ll\n",
    "    #standardize or normalize only on this fraction of the data.\n",
    "    \n",
    "    # We’re going to use the first num_train_samples timesteps \n",
    "    #as training data, so we’ll\n",
    "    #standardize or normalize only on this fraction of \n",
    "    #the data.i_dfs=li_vectorized_dfs,\n",
    "    \n",
    "    \n",
    "    #rep_stand_arrays= \n",
    "    #[standardized data array, \n",
    "    #number_train_samples,\n",
    "    #number_val_samples,\n",
    "    #number_test_samples,\n",
    "    #mean,\n",
    "    #std]\n",
    "    \n",
    "     \n",
    "    \n",
    "    rep_stand_arrays=fct_stand_single_df(\n",
    "    val_name_df=val_vectorized_df,\n",
    "    val_proportion_train_set=var_proportion_train_set,\n",
    "    val_proportion_val_set=var_proportion_val_set)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"size of the list with the number of training data \\\n",
    "    for each standardized array: \",\\\n",
    "          len(rep_stand_arrays[0]))\n",
    "    print(\"size of the list with the number of validation data\\\n",
    "    for each standardized array: \"\\\n",
    "          ,rep_stand_arrays[1])\n",
    "    print(\"size of the list with the number of test data \\\n",
    "    for each standardized array: \",\\\n",
    "          rep_stand_arrays[2])\n",
    "    print(\"**************************\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return [rep_stand_arrays,\\\n",
    "            val_li_targets_df,\\\n",
    "            val_mean_observations_per_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbdd4822-e5f1-43b1-ac4d-1009360e6366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create train, validation and test sets\n",
    "\n",
    "\n",
    "#function returning the train, validation and test dataset\n",
    "\n",
    "#we will use s_length_train timesteps to predict the next\n",
    "#s_length_target timesteps\n",
    "\n",
    "#data =data array,\n",
    "#target= target array,\n",
    "#num_train_samples= the number of training samples  (of the data),\n",
    "#num_val_samples= the number of validation samples,\n",
    "#num_test_samples = the number of test samples\n",
    "\n",
    "\n",
    "def fct_create_train_val_test_datasets_from_arrays(\n",
    "    s_length_train,\n",
    "    s_length_target,\n",
    "    s_stride,\n",
    "    b_size,\n",
    "    data,\n",
    "    target,\n",
    "    num_train_samples,\n",
    "    num_val_samples,\n",
    "    shuffle_tr_s,\n",
    "    shuffle_tr_t,\n",
    "    shuffle_v_s,\n",
    "    shuffle_v_t,\n",
    "    shuffle_t_s,\n",
    "    shuffle_t_t):\n",
    "    #,\n",
    "    #num_test_samples):\n",
    "    \n",
    "    \n",
    "    #the target  for a sequence\n",
    "    #will be the target s_length_target timesteps after the end of the sequence\n",
    "    \n",
    "    delay_tr=s_length_train\n",
    "    \n",
    "    delay_tar=s_length_target\n",
    "    \n",
    "    \n",
    "    #print(\"s_length_train\",s_length_train)\n",
    "    #print()\n",
    "    \n",
    "    #train set (batch size, timesteps, input_features)\n",
    "    #all timesteps  should belong in the train set so we must not have\n",
    "    #samples for which the target does not belong in the train set\n",
    "    input_train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    data, None, sequence_length=s_length_train, sequence_stride=s_stride,\n",
    "    start_index=0,end_index=num_train_samples-delay_tar,batch_size=b_size,\n",
    "    shuffle=shuffle_tr_s)\n",
    "    \n",
    "    #print(\"here1\")\n",
    "\n",
    "    #target of the train set\n",
    "    target_train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    target, None, sequence_length=s_length_target, sequence_stride=s_stride,\n",
    "    start_index=s_length_train,\n",
    "    end_index=num_train_samples,\n",
    "    batch_size=b_size,\n",
    "    shuffle=shuffle_tr_t)\n",
    "\n",
    "    #train-target dataset\n",
    "    train_dataset=tensorflow.data.Dataset.zip((input_train_dataset, target_train_dataset))\n",
    "\n",
    "    \n",
    "    #a=0\n",
    "    #print(len(train_dataset))\n",
    "    #for i,j in train_dataset:\n",
    "    #    print(\"input\",i)\n",
    "    #    print(\"target\",j)\n",
    "    #    a+=1\n",
    "    #    if a==73:\n",
    "    #    break \n",
    "    \n",
    "    #we create validation sets if the num_val_samples>0\n",
    "    if num_val_samples>0:\n",
    "        #validation sets\n",
    "        input_val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data, None, sequence_length=s_length_train, sequence_stride=s_stride,\n",
    "        start_index=num_train_samples,\n",
    "        end_index=num_train_samples+num_val_samples-delay_tar,\n",
    "        batch_size=b_size,\n",
    "        shuffle=shuffle_v_s)\n",
    "\n",
    "    \n",
    "    \n",
    "        #targets of the validation sets\n",
    "        target_val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        target,None, \n",
    "        sequence_length=s_length_target, sequence_stride=s_stride,\n",
    "        start_index=num_train_samples+s_length_train,\n",
    "        end_index=num_train_samples+num_val_samples,\n",
    "        batch_size=b_size,\n",
    "        shuffle=shuffle_v_t)\n",
    "\n",
    "    \n",
    "        #validation-target dataset\n",
    "        val_dataset=tensorflow.data.Dataset.zip((input_val_dataset,target_val_dataset))\n",
    "        \n",
    "    else:\n",
    "        val_dataset=None\n",
    "    \n",
    "    #print('start_index, end_index=',num_train_samples+s_length_train,\n",
    "    #num_train_samples+num_val_samples)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #test set\n",
    "    input_test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    data, None, sequence_length=s_length_train, sequence_stride=s_stride,\n",
    "    start_index=num_train_samples+num_val_samples,\n",
    "    end_index=data.shape[0]-delay_tar,\n",
    "    batch_size=b_size,\n",
    "    shuffle=shuffle_t_s)\n",
    "\n",
    "    #targets of the test set\n",
    "    target_test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    target, None, sequence_length=s_length_target, sequence_stride=s_stride,\n",
    "    start_index=num_train_samples+num_val_samples+s_length_train,\n",
    "    batch_size=b_size,\n",
    "    shuffle=shuffle_t_t)\n",
    "\n",
    "    \n",
    "    #test-target sets\n",
    "    test_dataset=tensorflow.data.Dataset.zip((input_test_dataset, target_test_dataset)) \n",
    "    \n",
    "    return train_dataset,val_dataset,test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98382c36-a4a9-4f0b-8f47-5fda3ec2d8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fct returning\n",
    "#train_dataset,\n",
    "#val_dataset,\n",
    "#test_dataset,\n",
    "#v_s_length_train,\n",
    "#stand_array.shape[-1] (nb feaures)\n",
    "#mean value of train data\n",
    "#std f train data\n",
    "#num train data\n",
    "#num validation data\n",
    "#sequence length target \n",
    "\n",
    "\n",
    "#va_dataframe=df_2009_1\n",
    "#va_folder_figure=the folder with the plots\n",
    "\n",
    "#if val_s_length_train then\n",
    "\n",
    "#val_s_length_train=\n",
    "#val_nb_past_seq_lengths x mean nb obesrvations per day\n",
    "\n",
    "#if val_s_length_target=None\n",
    "#val_s_length_target=\n",
    "#val_nb_future_seq_lengths x mean nb obesrvations per day\n",
    "\n",
    "#we  learn from val_s_length_train tsteps to predict\n",
    "#val_s_length_target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fct_create_train_val_test_datasets_from_dataframe(\n",
    "    val_dataframe,\n",
    "    val_s_stride,\n",
    "    val_b_size,\n",
    "    val_folder_figure,\n",
    "    val_nb_past_seq_lengths=1,\n",
    "    val_nb_future_seq_lengths=1,\n",
    "    val_s_length_train=None,\n",
    "    val_s_length_target=None,\n",
    "    val_name_col_origin=\"ORIGIN\",\n",
    "    val_li_name_col_to_copy=['DEST'],\n",
    "    val_name_column_date='FL_DATE',\n",
    "    val_name_col_2_sort='DEP_TIME',\n",
    "    val_name_target_variable=\"ARR_DELAY\",\n",
    "    val_li_cols_to_ignore=['FL_DATE','ORIGIN'],\n",
    "    val_proportion_train_set=0.5,\n",
    "    val_proportion_val_set=0.25,\n",
    "    val_name_fig=\"fig_cor_vector_data_heatmap.png\",\n",
    "    val_shuffle_tr_s=False,\n",
    "    val_shuffle_tr_t=False,\n",
    "    val_shuffle_v_s=False,\n",
    "    val_shuffle_v_t=False,\n",
    "    val_shuffle_t_s=False,\n",
    "    val_shuffle_t_t=False):\n",
    "    \n",
    "    #li_rep_stand_data=[rep_stand_arrays,\\\n",
    "    #val_li_targets_df,\\\n",
    "    #val_mean_observations_per_day]\n",
    "\n",
    "\n",
    "    #rep_stand_arrays= \n",
    "    #[standardized data array, \n",
    "    #number_train_samples,\n",
    "    #number_val_samples,\n",
    "    #number_test_samples,\n",
    "    #mean,\n",
    "    #std]\n",
    "    \n",
    "    li_rep_stand_data=fct_creation_data_arrays_model_2(\n",
    "    var_dataframe=val_dataframe,\n",
    "    var_folder_figure=val_folder_figure,\n",
    "    var_name_col_origin=val_name_col_origin,\n",
    "    var_li_name_col_to_copy=val_li_name_col_to_copy,\n",
    "    var_name_column_date=val_name_column_date,\n",
    "    var_name_col_2_sort=val_name_col_2_sort,\n",
    "    var_name_target_variable=val_name_target_variable,\n",
    "    var_li_cols_to_ignore=val_li_cols_to_ignore,\n",
    "    var_proportion_train_set=val_proportion_train_set,\n",
    "    var_proportion_val_set=val_proportion_val_set,\n",
    "    var_name_fig=val_name_fig)\n",
    "    \n",
    "    stand_array=li_rep_stand_data[0][0]\n",
    "    \n",
    "    #if we want to use the mean observations per day for\n",
    "    #learning from past tsteps to estimate\n",
    "    if val_s_length_train==None:\n",
    "        \n",
    "        mean_observ_per_day=\\\n",
    "        fct_measure_mean_observations_per_day_single_df(\n",
    "        val_name_df=val_dataframe,\n",
    "        val_name_column_date=val_name_column_date)\n",
    "        \n",
    "        v_s_length_train=\\\n",
    "        round(val_nb_past_seq_lengths*mean_observ_per_day)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        v_s_length_train=val_s_length_train\n",
    "        mean_observ_per_day=None\n",
    "    \n",
    "    #if we want to use the mean observations per day for\n",
    "    #the future tsteps to estimate\n",
    "    if val_s_length_target==None:\n",
    "        \n",
    "        if mean_observ_per_day==None:\n",
    "            mean_observ_per_day=\\\n",
    "            fct_measure_mean_observations_per_day_single_df(\n",
    "            val_name_df=val_dataframe,\n",
    "            val_name_column_date=val_name_column_date)\n",
    "        \n",
    "        v_s_length_target=\\\n",
    "        round(mean_observ_per_day*val_nb_future_seq_lengths)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        v_s_length_target=val_s_length_target\n",
    "    \n",
    "    \n",
    "    #we define the train, validation and tests sets\n",
    "    train_dataset,val_dataset,test_dataset=\\\n",
    "    fct_create_train_val_test_datasets_from_arrays(\n",
    "    s_length_train=v_s_length_train,\n",
    "    s_length_target=v_s_length_target,\n",
    "    s_stride=val_s_stride,\n",
    "    b_size=val_b_size,\n",
    "    data=stand_array,\n",
    "    target=li_rep_stand_data[1],\n",
    "    num_train_samples=li_rep_stand_data[0][1],\n",
    "    num_val_samples=li_rep_stand_data[0][2],\n",
    "    shuffle_tr_s=val_shuffle_tr_s,\n",
    "    shuffle_tr_t=val_shuffle_tr_t,\n",
    "    shuffle_v_s=val_shuffle_v_s,\n",
    "    shuffle_v_t=val_shuffle_v_t,\n",
    "    shuffle_t_s=val_shuffle_t_s,\n",
    "    shuffle_t_t=val_shuffle_t_t)\n",
    "    \n",
    "    return train_dataset,val_dataset,test_dataset,v_s_length_train,stand_array.shape[-1],\\\n",
    "    li_rep_stand_data[0][4],li_rep_stand_data[0][5],li_rep_stand_data[0][1],\\\n",
    "    li_rep_stand_data[0][2],v_s_length_target\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6012b6-8772-4c3c-8f6f-5bb0a10e959e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb6ebb2b-929f-41b5-9478-3183e5b62552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function which creates a model for the hyperparameters hp\n",
    "#best hp)\n",
    "#and finds the best number of epochs  to retrain the best models\n",
    "#t returns the best epoch and the history object when searching\n",
    "#the best epoch of the best model\n",
    "\n",
    "#this function takes as argument the batch size but\n",
    "#since data are already provided per batches\n",
    "#it is not used in the \n",
    "#fir metod. To examine if we should  use it\n",
    "\n",
    "\n",
    "def fct_get_best_epoch(\n",
    "    va_hp,\n",
    "    va_tuner,\n",
    "    va_train_dataset,\n",
    "    va_val_dataset,\n",
    "    va_batch_size=None,\n",
    "    va_metric_to_monitor_best_epoch_callbacks=\"val_loss\", \n",
    "    va_mode_callbacks=\"min\", \n",
    "    va_patience_best_epoch_callbacks=10,\n",
    "    va_epochs=2):\n",
    "        \n",
    "        print(\" IN FCT BEST_EPOCH, WE START SEARCH BEST EPOCH FOR HP\")\n",
    "    \n",
    "    \n",
    "        model = va_tuner.hypermodel.build(va_hp)\n",
    "    \n",
    "        print(\"In fct best_epoch, va_metric_to_monitor_best_epoch_callbacks\",\\\n",
    "        va_metric_to_monitor_best_epoch_callbacks)\n",
    "        print()\n",
    "    \n",
    "        #Typically, we want to train the new models \n",
    "        #for longer than we did during the search: \n",
    "        #using an aggressive patience value(=10)  in the \n",
    "        #EarlyStopping callback saves time during the search, \n",
    "        #but it may lead to under-fit models. \n",
    "        #Thus we use the validation set to find the best epoch:\n",
    "   \n",
    "        #va_metric_to_monitor_best_epoch_callbacks wil lbe a metric of the validation dataset\n",
    "        callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "        monitor=va_metric_to_monitor_best_epoch_callbacks, \n",
    "        mode=va_mode_callbacks, \n",
    "        patience=va_patience_best_epoch_callbacks)\n",
    "        ]\n",
    "    \n",
    "        #fit the model,\n",
    "        #model.fit returns a History object\n",
    "        #which records metrics (train and/or validation set) for each epoch\n",
    "        #It containing a dictionary member named history key=metric\n",
    "        #value=records per epoch\n",
    "        history = model.fit(\n",
    "        va_train_dataset,\n",
    "        validation_data=va_val_dataset,\n",
    "        epochs=va_epochs,\n",
    "        #batch_size=va_batch_size,\n",
    "        callbacks=callbacks)\n",
    "    \n",
    "        #print(\"history\",history)\n",
    "        #print(\"history.keys\", history.history.keys())\n",
    "        #for  i in history.history.keys():\n",
    "        #    print(history.history[i])\n",
    "        #    import sys\n",
    "         #   sys.exit()\n",
    "    \n",
    "        #history.history[\"loss\"]=[ training loss 1st epoch, training loss 2nd epoch...]\n",
    "    \n",
    "        #tha validation loss per epoch\n",
    "        val_loss_per_epoch = history.history[va_metric_to_monitor_best_epoch_callbacks]\n",
    "    \n",
    "        print(\"In fct best_epoch, val_loss_per_epoch-result history:\",val_loss_per_epoch)\n",
    "        print()\n",
    "    \n",
    "        best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
    "    \n",
    "        print(f\"Best epoch: {best_epoch}\")\n",
    "        print()\n",
    "        print(\"END FCT BEST EPOCH\")\n",
    "        \n",
    "        return best_epoch, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66fef8-3fb7-449a-8072-a55def57fcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf84e4a4-3421-47e5-83e8-8d366b9c49f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which creates a model from a given set of hyperparameters\n",
    "#finds the best number of epochs\n",
    "#retraines the model\n",
    "#returns the best trained model \n",
    "\n",
    "#va_train_dataset= the train dataset\n",
    "#va_new_train=the train +the validation dataset\n",
    "\n",
    "def fct_get_best_trained_model(\n",
    "    va_hp,\n",
    "    va_tuner,\n",
    "    va_train_dataset,\n",
    "    va_new_train,\n",
    "    va_val_dataset,\n",
    "    va_index_for_saving_best_model,\n",
    "    va_to_multiply_epoch_for_train_dur,\n",
    "    va_batch_size=None,\n",
    "    va_metric_to_monitor_best_epoch_callbacks=\"val_loss\", \n",
    "    va_mode_callbacks=\"min\", \n",
    "    va_patience_best_epoch_callbacks=10,\n",
    "    va_epochs=2,\n",
    "    va_pkl_filename_best_model = \"best_model\",\n",
    "    va_pkl_filename_best_retrained_model=\\\n",
    "    \"history_obj_best_retrained_model\"\n",
    "    ):\n",
    "        \n",
    "        print(\"WE START FCT BEST_TRAINED_MODEL BY SEARCHING BEST EPOCH\")\n",
    "    \n",
    "        #we find best epoch for the hyperparameters hp\n",
    "        best_epoch,history_when_search_best_epoch = fct_get_best_epoch(\n",
    "        va_hp=va_hp,\n",
    "        va_tuner=va_tuner,\n",
    "        va_train_dataset=va_train_dataset,\n",
    "        va_val_dataset=va_val_dataset,\n",
    "        va_batch_size=va_batch_size,\n",
    "        va_metric_to_monitor_best_epoch_callbacks=\\\n",
    "        va_metric_to_monitor_best_epoch_callbacks, \n",
    "        va_mode_callbacks=va_mode_callbacks, \n",
    "        va_patience_best_epoch_callbacks=\\\n",
    "        va_patience_best_epoch_callbacks,\n",
    "        va_epochs=va_epochs)\n",
    "\n",
    "        model = va_tuner.hypermodel.build(va_hp)\n",
    "    \n",
    "        #we save the best  model\n",
    "        fi_name=va_pkl_filename_best_model+\"_\"+\\\n",
    "        str(va_index_for_saving_best_model)+\".pkl\"\n",
    "\n",
    "        with open(fi_name, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "    \n",
    "        print(\"IN FCT BEST_TRAINED MODEL \\\n",
    "        WE CREATED MODEL WITH BEST HP - WE WILL START RETRAIN IT\\\n",
    "        FOR A LITTLE LONGER THAN THE BEST NUMBER OF EPOCHS \\\n",
    "        IN TRAIN+VAL SET\")\n",
    "          \n",
    "        #we retrain the (best) model for a little  longer \n",
    "        #than the best number of  epochs\n",
    "        #since we have more  data (va_new_train=train+validation)\n",
    "        \n",
    "        #history=the values of each metric per epoch\n",
    "        #for the train set we don't use the validation set)\n",
    "        history_retrained_model=model.fit(\n",
    "        va_new_train,\n",
    "        #batch_size=va_batch_size, \n",
    "        epochs=int(best_epoch * \n",
    "        va_to_multiply_epoch_for_train_dur))\n",
    "    \n",
    "        #we save the history object of the best retrained model\n",
    "        fi_name_1=va_pkl_filename_best_retrained_model+\"_\"+\\\n",
    "        str(va_index_for_saving_best_model)+\".pkl\"\n",
    "\n",
    "        \n",
    "        with open(fi_name_1, 'wb') as file_1:\n",
    "            pickle.dump(history_retrained_model, file_1)\n",
    "        \n",
    "        print(\"IN FCT BEST_TRAINED_MODEL END RETRAINED MODEL-END FCT\")\n",
    "    \n",
    "        return model,history_when_search_best_epoch,history_retrained_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3eaa5-305e-4903-b635-0f7a8c502b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a387b5f3-f727-44d8-be99-b8e0404394d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method searching for the best model using a BO tuner\n",
    "\n",
    "#1.we find the best hyperparameters using a BO tuner\n",
    "#2.we find the best number of epochs \n",
    "#3.we  return the best trained model\n",
    "\n",
    "#in tuners we should Always \n",
    "#specify validation metrics, \n",
    "#since the goal of the search process \n",
    "#is to find models that generalize\n",
    "#so \n",
    "#val_objective_metric_for_tuner_to_optimize,\n",
    "#val_metric_for_tuner_callback,\n",
    "#val_monitor_callbacks\n",
    "#are metrics for the validation dataset\n",
    "\n",
    "#val_max_trials=Maximum number of different model configurations \n",
    "#(“trials”) to try before ending the search\n",
    "\n",
    "#To reduce metrics variance, we can  train the\n",
    "#same model multiple times and average the results. \n",
    "#val_executions_per_trial is how many \n",
    "#training rounds (executions) to run \n",
    "#for each model configuration (trial).\n",
    "\n",
    "#val_directory=Where\n",
    "#to store search logs\n",
    "\n",
    "#val_overwrite=Whether to overwrite data in directory \n",
    "#to start a new search.\n",
    "\n",
    "def fct_search_best_model_using_tuner(\n",
    "val_di_tuners,\n",
    "val_key_tuner_class,\n",
    "val_hypermodel,\n",
    "val_objective_metric_for_tuner_to_optimize,\n",
    "val_mode,\n",
    "val_max_trials,\n",
    "val_executions_per_trial,\n",
    "val_directory,\n",
    "val_metric_for_tuner_search_hp_callback,\n",
    "val_li_keys_tuners_optimizing_batch_size,\n",
    "val_train_dataset,\n",
    "val_val_dataset,\n",
    "val_test_dataset,\n",
    "val_epochs_tuner_search,\n",
    "val_top_best_models,\n",
    "val_batch_size,\n",
    "val_to_multiply_epoch_for_train_dur,\n",
    "val_metric_to_monitor_best_epoch_callbacks,\n",
    "val_epochs_best_trained_model_search,\n",
    "val_overwrite=True,\n",
    "val_patience_during_tuner_search=5,\n",
    "val_verbose=2,\n",
    "val_mode_callbacks=\"min\",\n",
    "val_patience_best_epoch_callbacks=10,\n",
    "val_pkl_filename_best_model = \"best_model\",\n",
    "val_pkl_filename_best_retrained_model=\\\n",
    "\"history_obj_best_retrained_model\"):\n",
    "   \n",
    "\n",
    "    print(\"IN FCT fct_search_best_model_using_tuner WE CREATE A TUNER\\\n",
    "    TO SEARCH THE BEST MODEL\")\n",
    "\n",
    "    #we define the tuner\n",
    "    tuner = val_di_tuners[val_key_tuner_class](\n",
    "    hypermodel=val_hypermodel,\n",
    "    objective=\\\n",
    "    kt.Objective(val_objective_metric_for_tuner_to_optimize,\\\n",
    "                direction=val_mode),\n",
    "    max_trials=val_max_trials,\n",
    "    executions_per_trial=val_executions_per_trial,\n",
    "    directory=val_directory,\n",
    "    overwrite=val_overwrite)\n",
    "    \n",
    "    print(\"IN FCT fct_search_best_model_using_tuner \\\n",
    "    val_metric_for_tuner_search_hp_callback\",\n",
    "          val_metric_for_tuner_search_hp_callback)\n",
    "    print()\n",
    "\n",
    "    #we print a summary of the hyperparameters in the search space\n",
    "    #that is\n",
    "    #the number of hyperparameters we search to optimize (7)\n",
    "    #number layers\n",
    "    #units each layer\n",
    "    #activation function each layer\n",
    "    #dropout\n",
    "    #activation function last layer\n",
    "    #optimizer\n",
    "    #learining rate etc.\n",
    "    print()\n",
    "    print(\"IN fct_search_best_model_using_tuner \\\n",
    "    Summary of the hyperparameters in the search space\",\\\n",
    "    tuner.search_space_summary())\n",
    "    print()\n",
    "\n",
    "    #At each trial, \n",
    "    #the tuner would generate a new set of hyperparameter values \n",
    "    # build the model\n",
    "    #Train the model and record its metric\n",
    "\n",
    "    #we start the search of best hyperparameters\n",
    "\n",
    "    #callback.EarlyStopping= prematurely stop training \n",
    "    #if validation loss doesn’t improve\n",
    "    #that is when it starts overfit\n",
    "    callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\\\n",
    "    monitor=val_metric_for_tuner_search_hp_callback, \n",
    "    patience=val_patience_during_tuner_search),\n",
    "    ]\n",
    "    #if the tuner optimizes the batch size\n",
    "    if val_key_tuner_class in val_li_keys_tuners_optimizing_batch_size:\n",
    "        \n",
    "        tuner.search(\n",
    "        val_train_dataset,\n",
    "        #batch_size=val_batch_size,\n",
    "        epochs=val_epochs_tuner_search,\n",
    "        validation_data=val_val_dataset,\n",
    "        callbacks=callbacks,\n",
    "        verbose=val_verbose)\n",
    "\n",
    "    #if  the tuner doesn't optimize the batch size\n",
    "    else:\n",
    "        #we perform a search for best hyperparameter configurations\n",
    "        tuner.search(\n",
    "        val_train_dataset,\n",
    "        batch_size=val_batch_size,\n",
    "        epochs=val_epochs_tuner_search,\n",
    "        validation_data=val_val_dataset,\n",
    "        callbacks=callbacks,\n",
    "        verbose=val_verbose)\n",
    "\n",
    "    print()\n",
    "    print(\"Results Summary of the tuner\",tuner.results_summary())\n",
    "    print()\n",
    "\n",
    "    print(\"IN FCT fct_search_best_model_using_tuner,\\\n",
    "    END TUNER SEARCH FOR FINDING BEST MODEL\")\n",
    "    print()\n",
    "\n",
    "    print(\"In FCT fct_search_best_model_using_tuner,\\\n",
    "    val_top_best_models\",val_top_best_models)\n",
    "    print()\n",
    "\n",
    "    \n",
    "    #the best hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(val_top_best_models)\n",
    "\n",
    "    #for each best hp we search the best epoch to retrain the best model\n",
    "\n",
    "\n",
    "    #Usually, when retraining these models,\n",
    "    #we  include the validation data as part of the training data, \n",
    "    #since we won’t be making any further hyperparameter changes, a\n",
    "    #nd thus we will no longer be evaluating performance on the validation data.\n",
    "    \n",
    "    v_new_train=val_train_dataset.concatenate(val_val_dataset)\n",
    "\n",
    "\n",
    "    #we create the best trained models \n",
    "    #for each considered set of best hyperparameters\n",
    "    #di_best_models dict, key=id best model (start with 1)\n",
    "    #value=best model\n",
    "    di_best_trained_models = {}\n",
    "    \n",
    "    #di_hist_when_search_best_epoch=dictionary,\n",
    "    #key=id best model (starting with 1)\n",
    "    #value=history when searching best epoch\n",
    "    di_hist_when_search_best_epoch={}\n",
    "    \n",
    "    #di_hist_retrained_best_model=dictionary,\n",
    "    #key=id best model (starting with 1)\n",
    "    #value=history retrained best model\n",
    "    #this history cotnains values for both train and\n",
    "    #validation sets\n",
    "    di_hist_retrained_best_model={}\n",
    "    \n",
    "    #di_results_model_eval_test_set=dictionary,\n",
    "    #key=id best model (starting with 1)\n",
    "    #value=dictkey=id metric, value=value metric\n",
    "    di_results_model_eval_test_set={}\n",
    "    \n",
    "    indice=0\n",
    "\n",
    "    #for each best hyperparameter:\n",
    "    #i. we create a best model\n",
    "    #ii. we find the best epoch\n",
    "    #iii. we retrain the best model for the best epoch\n",
    "    for hp in best_hps:\n",
    "        \n",
    "        indice+=1\n",
    "        \n",
    "        print()\n",
    "        print(\"IN FCT fct_search_best_model_using_tuner,\\\n",
    "        hyperparameter number:\",indice)\n",
    "        print(\"IN FCT fct_search_best_model_using_tunerr,\\\n",
    "        we  will search for the best trained model THAT IS THE BEST EPOCH\\\n",
    "        AND RETRAIN THE BEST  MODEL using function \\\n",
    "        get_best_trained_model\") \n",
    "\n",
    "        model,history_when_search_best_epoch,history_retrained_model=\\\n",
    "        fct_get_best_trained_model(\n",
    "        va_hp=hp,\n",
    "        va_tuner=tuner,\n",
    "        va_train_dataset=val_train_dataset,\n",
    "        va_new_train=v_new_train,\n",
    "        va_val_dataset=val_val_dataset,\n",
    "        va_index_for_saving_best_model=indice,\n",
    "        va_to_multiply_epoch_for_train_dur=\\\n",
    "        val_to_multiply_epoch_for_train_dur,\n",
    "        va_batch_size=val_batch_size,\n",
    "        va_metric_to_monitor_best_epoch_callbacks=\\\n",
    "        val_metric_to_monitor_best_epoch_callbacks,\n",
    "        va_mode_callbacks=val_mode_callbacks, \n",
    "        va_patience_best_epoch_callbacks=\\\n",
    "        val_patience_best_epoch_callbacks,\n",
    "        va_epochs=val_epochs_best_trained_model_search,\n",
    "        va_pkl_filename_best_model = val_pkl_filename_best_model,\n",
    "        va_pkl_filename_best_retrained_model=\\\n",
    "        val_pkl_filename_best_retrained_model\n",
    "        )\n",
    "\n",
    "        di_best_trained_models[indice]=model\n",
    "                \n",
    "        di_hist_when_search_best_epoch[indice]=history_when_search_best_epoch\n",
    "        \n",
    "        di_hist_retrained_best_model[indice]=history_retrained_model\n",
    "\n",
    "        #model.evaluate returns a list of scalars for the loss and metrics\n",
    "        #for the test set\n",
    "        #results=[value of test_loss, value of test_mae, values of test_rmse]\n",
    "        results_model_eval_test_set=model.evaluate(val_test_dataset)\n",
    "        \n",
    "        di_results_model_eval_test_set[indice]={}\n",
    "\n",
    "        li_metric_names=model.metrics_names\n",
    "        nb_metrics=len(li_metric_names)\n",
    "        for j in range(nb_metrics):\n",
    "            di_results_model_eval_test_set[indice][li_metric_names[j]]=\\\n",
    "            results_model_eval_test_set[j]\n",
    "        \n",
    "    for i in di_hist_when_search_best_epoch:\n",
    "        print()\n",
    "        print(\"Search Best Epoch For Best retrained model :\", i)\n",
    "        #for each metric\n",
    "        for j in di_hist_when_search_best_epoch[i].history:\n",
    "            print(\"metric\",j)\n",
    "            print(\"values per epoch:\",di_hist_when_search_best_epoch[i].history[j])\n",
    "      \n",
    "    \n",
    "    for i in di_hist_retrained_best_model:\n",
    "        print()\n",
    "        print(\"Evaluation Best retrained model :\", i)\n",
    "        #for each metric\n",
    "        for j in di_hist_retrained_best_model[i].history:\n",
    "            print(\"metric\",j)\n",
    "            print(\"values per epoch:\",di_hist_retrained_best_model[i].history[j])\n",
    "            \n",
    "    for i in di_results_model_eval_test_set:\n",
    "        print()\n",
    "        print(\"Evaluation Test Set: Best retrained model :\", i)\n",
    "        #for each metric\n",
    "        for j in di_results_model_eval_test_set[i]:\n",
    "            print(\"Test metric\",j)\n",
    "            print(\"Test metric value:\",di_results_model_eval_test_set[i][j])\n",
    "            \n",
    "\n",
    "    print(\"FIN\")\n",
    "    \n",
    "    return di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "    di_hist_retrained_best_model,di_results_model_eval_test_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e9180-ea03-4e39-abc3-be6d3116c9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5feb1c92-a930-4c3f-b1dd-cfe5f1c10e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returning the best model(s) performance when\n",
    "# a single study involving all origins is considered\n",
    "#it returns\n",
    "#di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "#di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "#va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "#va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "#va_name_figure_folder_metric,va_name_figure_metric\n",
    "\n",
    "def fct_best_approaches_1(\n",
    "    v_dataframe,\n",
    "    v_s_stride,\n",
    "    v_b_size,\n",
    "    v_folder_figures,\n",
    "    v_nb_past_seq_lengths,\n",
    "    v_nb_future_seq_lengths,\n",
    "    v_s_length_train_model,\n",
    "    v_s_length_target,\n",
    "    v_name_col_origin,\n",
    "    v_li_name_col_to_copy,\n",
    "    v_name_column_date,\n",
    "    v_name_col_2_sort,\n",
    "    v_name_target_variable,\n",
    "    v_li_cols_to_ignore,\n",
    "    v_proportion_train_set,\n",
    "    v_proportion_val_set,\n",
    "    v_name_fig,\n",
    "    v_shuffle_tr_s,\n",
    "    v_shuffle_tr_t,\n",
    "    v_shuffle_v_s,\n",
    "    v_shuffle_v_t,\n",
    "    v_shuffle_t_s,\n",
    "    v_shuffle_t_t,\n",
    "    v_di_hypermodels,\n",
    "    v_key_hypermodel_class,\n",
    "    v_min_nb_lay_model,\n",
    "    v_max_nb_lay_model,\n",
    "    v_min_nb_units_model,\n",
    "    v_max_nb_units_model,\n",
    "    v_min_value_dropout_rate_model,\n",
    "    v_max_value_dropout_rate_model,\n",
    "    v_min_value_recurrent_dropout_rate_model,\n",
    "    v_max_value_recurrent_dropout_rate_model,\n",
    "    v_min_nb_filters_conv1d,\n",
    "    v_max_nb_filters_conv1d,\n",
    "    v_min_nb_kernel_size_conv1d,\n",
    "    v_max_nb_kernel_size_conv1d,\n",
    "    v_step_nb_layers_model,\n",
    "    v_step_nb_units_model,\n",
    "    v_step_dropout_rate_model,\n",
    "    v_step_recurrent_dropout_rate_model,\n",
    "    v_step_nb_kernel_size_conv1d,\n",
    "    v_min_pool_size,\n",
    "    v_max_pool_size,\n",
    "    v_step_pool_size,\n",
    "    v_li_activ_fcts_model,\n",
    "    v_li_optimizers_model,\n",
    "    v_min_val_learning_rate_optimizer,\n",
    "    v_max_val_learning_rate_optimizer,\n",
    "    v_loss_fct_model,\n",
    "    v_metrics_model,\n",
    "    v_di_tuners,\n",
    "    v_key_tuner_class,\n",
    "    v_objective_metric_for_tuner_to_optimize,\n",
    "    v_mode,\n",
    "    v_max_trials,\n",
    "    v_executions_per_trial,\n",
    "    v_directory,\n",
    "    v_metric_for_tuner_search_hp_callback,\n",
    "    v_li_keys_tuners_optimizing_batch_size,\n",
    "    v_epochs_tuner_search,\n",
    "    v_top_best_models,\n",
    "    v_batch_size,\n",
    "    v_to_multiply_epoch_for_train_dur,\n",
    "    v_metric_to_monitor_best_epoch_callbacks,\n",
    "    v_epochs_best_trained_model_search,\n",
    "    v_overwrite=True,\n",
    "    v_patience_during_tuner_search=5,\n",
    "    v_verbose=2,\n",
    "    v_mode_callbacks=\"min\",\n",
    "    v_patience_best_epoch_callbacks=10,\n",
    "    v_pkl_filename_best_model = \"best_model\",\n",
    "    v_pkl_filename_best_retrained_model=\\\n",
    "    \"history_obj_best_retrained_model\"\n",
    "):\n",
    "\n",
    "    #create the folder for the figures (plots)\n",
    "    #folder for the all the figure plots\n",
    "    os.makedirs(v_folder_figures,exist_ok = True)\n",
    "\n",
    "    \n",
    "    #create data arrays\n",
    "    train_dataset,val_dataset,test_dataset,\\\n",
    "    v_s_length_train,nb_input_features,\\\n",
    "    mean_val_train_data,std_val_train_data,\\\n",
    "    number_train_data,number_validation_data,\\\n",
    "    v_s_length_target=\\\n",
    "    fct_create_train_val_test_datasets_from_dataframe(\n",
    "    val_dataframe=v_dataframe,\n",
    "    val_s_stride=v_s_stride,\n",
    "    val_b_size=v_b_size,\n",
    "    val_folder_figure=v_folder_figures,\n",
    "    val_nb_past_seq_lengths=v_nb_past_seq_lengths,\n",
    "    val_nb_future_seq_lengths=v_nb_future_seq_lengths,\n",
    "    val_s_length_train=v_s_length_train_model,\n",
    "    val_s_length_target=v_s_length_target,\n",
    "    val_name_col_origin=v_name_col_origin,\n",
    "    val_li_name_col_to_copy=v_li_name_col_to_copy,\n",
    "    val_name_column_date=v_name_column_date,\n",
    "    val_name_col_2_sort=v_name_col_2_sort,\n",
    "    val_name_target_variable=v_name_target_variable,\n",
    "    val_li_cols_to_ignore=v_li_cols_to_ignore,\n",
    "    val_proportion_train_set=v_proportion_train_set,\n",
    "    val_proportion_val_set=v_proportion_val_set,\n",
    "    val_name_fig=v_name_fig,\n",
    "    val_shuffle_tr_s=v_shuffle_tr_s,\n",
    "    val_shuffle_tr_t=v_shuffle_tr_t,\n",
    "    val_shuffle_v_s=v_shuffle_v_s,\n",
    "    val_shuffle_v_t=v_shuffle_v_t,\n",
    "    val_shuffle_t_s=v_shuffle_t_s,\n",
    "    val_shuffle_t_t=v_shuffle_t_t)\n",
    "    \n",
    "\n",
    "    v_nb_initial_input_features_model=nb_input_features\n",
    "\n",
    "    #v_min_nb_units_model=int(nb_input_features+1)\n",
    "\n",
    "    #the sequence length of the hypermodel will be the value returned \n",
    "    #by the function fct_create_train_val_test_datasets_from_dataframe\n",
    "    #that is \n",
    "    #val_s_length_train_model=v_s_length_train\n",
    "    #the same will hold true  from now on \n",
    "    #whenever we need the sequence length of the train set\n",
    "    #s_length_train\n",
    "    #The number of outputs of the  last ayer of the model willbe\n",
    "    #the number of tsteps to predict as we are dealing with a \n",
    "    #timeseries forecast problem\n",
    "    hypermodel_j=v_di_hypermodels[v_key_hypermodel_class](\n",
    "    val_s_length_train_model=v_s_length_train,\n",
    "    val_nb_initial_input_features_model=v_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=v_min_nb_lay_model,\n",
    "    val_max_nb_lay_model=v_max_nb_lay_model,\n",
    "    val_min_nb_units_model=v_min_nb_units_model,\n",
    "    val_max_nb_units_model=v_max_nb_units_model,\n",
    "    val_min_value_dropout_rate_model=v_min_value_dropout_rate_model,\n",
    "    val_max_value_dropout_rate_model=v_max_value_dropout_rate_model,\n",
    "    val_min_nb_filters_conv1d=v_min_nb_filters_conv1d,\n",
    "    val_max_nb_filters_conv1d=v_max_nb_filters_conv1d,\n",
    "    val_min_nb_kernel_size_conv1d=v_min_nb_kernel_size_conv1d,\n",
    "    val_max_nb_kernel_size_conv1d=v_max_nb_kernel_size_conv1d,\n",
    "    val_min_value_recurrent_dropout_rate_model=\\\n",
    "    v_min_value_recurrent_dropout_rate_model,\n",
    "    val_max_value_recurrent_dropout_rate_model=\\\n",
    "    v_max_value_recurrent_dropout_rate_model,\n",
    "    val_step_nb_layers_model=v_step_nb_layers_model,\n",
    "    val_step_nb_units_model=v_step_nb_units_model,\n",
    "    val_step_dropout_rate_model=v_step_dropout_rate_model,\n",
    "    val_step_recurrent_dropout_rate_model=\\\n",
    "    v_step_recurrent_dropout_rate_model,\n",
    "    val_step_nb_kernel_size_conv1d=\\\n",
    "    v_step_nb_kernel_size_conv1d,\n",
    "    val_min_pool_size=v_min_pool_size,\n",
    "    val_max_pool_size=v_max_pool_size,\n",
    "    val_step_pool_size=v_step_pool_size,\n",
    "    val_li_activ_fcts_model=v_li_activ_fcts_model,\n",
    "    val_nb_last_output_classes_model=v_s_length_target,\n",
    "    val_li_optimizers_model=v_li_optimizers_model,\n",
    "    val_min_val_learning_rate_optimizer=\\\n",
    "    v_min_val_learning_rate_optimizer,\n",
    "    val_max_val_learning_rate_optimizer=\\\n",
    "    v_max_val_learning_rate_optimizer,\n",
    "    var_loss_fct_model=v_loss_fct_model,\n",
    "    var_metrics_model=v_metrics_model)\n",
    "\n",
    "    v_hypermodel=hypermodel_j\n",
    "\n",
    "    di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "    di_hist_retrained_best_model,di_results_model_eval_test_set=\\\n",
    "    fct_search_best_model_using_tuner(\n",
    "    val_di_tuners=v_di_tuners,\n",
    "    val_key_tuner_class=v_key_tuner_class,\n",
    "    val_hypermodel=v_hypermodel,\n",
    "    val_objective_metric_for_tuner_to_optimize=\\\n",
    "    v_objective_metric_for_tuner_to_optimize,\n",
    "    val_mode=v_mode,\n",
    "    val_max_trials=v_max_trials,\n",
    "    val_executions_per_trial=v_executions_per_trial,\n",
    "    val_directory=v_directory,\n",
    "    val_metric_for_tuner_search_hp_callback=\\\n",
    "    v_metric_for_tuner_search_hp_callback,\n",
    "    val_li_keys_tuners_optimizing_batch_size=\\\n",
    "    v_li_keys_tuners_optimizing_batch_size,\n",
    "    val_train_dataset=train_dataset,\n",
    "    val_val_dataset=val_dataset,\n",
    "    val_test_dataset=test_dataset,\n",
    "    val_epochs_tuner_search=v_epochs_tuner_search,\n",
    "    val_top_best_models=v_top_best_models,\n",
    "    val_batch_size=v_batch_size,\n",
    "    val_to_multiply_epoch_for_train_dur=\\\n",
    "    v_to_multiply_epoch_for_train_dur,\n",
    "    val_metric_to_monitor_best_epoch_callbacks=\\\n",
    "    v_metric_to_monitor_best_epoch_callbacks,\n",
    "    val_epochs_best_trained_model_search=\\\n",
    "    v_epochs_best_trained_model_search,\n",
    "    val_overwrite=v_overwrite,\n",
    "    val_patience_during_tuner_search=v_patience_during_tuner_search,\n",
    "    val_verbose=v_verbose,\n",
    "    val_mode_callbacks=v_mode_callbacks,\n",
    "    val_patience_best_epoch_callbacks=\\\n",
    "    v_patience_best_epoch_callbacks,\n",
    "    val_pkl_filename_best_model=v_pkl_filename_best_model,\n",
    "    val_pkl_filename_best_retrained_model=\\\n",
    "    v_pkl_filename_best_retrained_model)\n",
    "\n",
    "\n",
    "    va_test_dataset=test_dataset\n",
    "    \n",
    "    #the mean value of  the target variable is the 7th column\n",
    "    #enumeration starting with zero as 2st element)\n",
    "    va_mean_value_train_dataset=mean_val_train_data[7]\n",
    "    \n",
    "    #the std value of  the target variable is the 7th column\n",
    "    #enumeration starting with zero as 2st element)\n",
    "    va_std_train_dataset=std_val_train_data[7]\n",
    "\n",
    "\n",
    "    va_id_first_future_observation=int(number_train_data+number_validation_data)\n",
    "    \n",
    "    va_li_true_vals_test_set=\\\n",
    "    v_dataframe[v_name_target_variable].values[va_id_first_future_observation:]\n",
    "    \n",
    "    va_name_figure_folder_metric=v_folder_figures\n",
    "\n",
    "    #print(\"va_name_figure_folder_metric\",va_name_figure_folder_metric)\n",
    "\n",
    "    #we will not use, it as we use a single folder \n",
    "    #the one named v_folder_figures\n",
    "    #tths variable should be removed\n",
    "    #va_name_figure_folder_metric=None\n",
    "    \n",
    "    va_name_figure_metric=\"fig_inferences\"\n",
    "    \n",
    "    li_rep=[di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "    di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "    va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "    va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "    va_name_figure_folder_metric,va_name_figure_metric]\n",
    "\n",
    "    \n",
    "    return li_rep\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c046fc9e-f8ae-4ca9-b4cb-4b03796ee8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function  returning the  best model(s) performance\n",
    "#when considering a distinct study for eash origin \n",
    "#it returns a dictionary\n",
    "#di_rep\n",
    "#val_key_model_class=the key  of the desired model \n",
    "#in the dictionary  di_hypermodels, defined in the \n",
    "#file with the Hyperparameter models\n",
    "#di[key]= the hypemodel we want to create\n",
    "\n",
    "\n",
    "def fct_best_approaches_2(\n",
    "    v_di_dataframes,\n",
    "    v_s_stride,\n",
    "    v_b_size,\n",
    "    v_folder_figures,\n",
    "    v_nb_past_seq_lengths,\n",
    "    v_nb_future_seq_lengths,\n",
    "    v_s_length_train_model,\n",
    "    v_s_length_target,\n",
    "    v_name_col_origin,\n",
    "    v_li_name_col_to_copy,\n",
    "    v_name_column_date,\n",
    "    v_name_col_2_sort,\n",
    "    v_name_target_variable,\n",
    "    v_li_cols_to_ignore,\n",
    "    v_proportion_train_set,\n",
    "    v_proportion_val_set,\n",
    "    v_name_fig,\n",
    "    v_shuffle_tr_s,\n",
    "    v_shuffle_tr_t,\n",
    "    v_shuffle_v_s,\n",
    "    v_shuffle_v_t,\n",
    "    v_shuffle_t_s,\n",
    "    v_shuffle_t_t,\n",
    "    v_di_hypermodels,\n",
    "    v_key_hypermodel_class,\n",
    "    v_min_nb_lay_model,\n",
    "    v_max_nb_lay_model,\n",
    "    v_min_nb_units_model,\n",
    "    v_max_nb_units_model,\n",
    "    v_min_value_dropout_rate_model,\n",
    "    v_max_value_dropout_rate_model,\n",
    "    v_min_value_recurrent_dropout_rate_model,\n",
    "    v_max_value_recurrent_dropout_rate_model,\n",
    "    v_min_nb_filters_conv1d,\n",
    "    v_max_nb_filters_conv1d,\n",
    "    v_min_nb_kernel_size_conv1d,\n",
    "    v_max_nb_kernel_size_conv1d,\n",
    "    v_step_nb_layers_model,\n",
    "    v_step_nb_units_model,\n",
    "    v_step_dropout_rate_model,\n",
    "    v_step_recurrent_dropout_rate_model,\n",
    "    v_step_nb_kernel_size_conv1d,\n",
    "    v_min_pool_size,\n",
    "    v_max_pool_size,\n",
    "    v_step_pool_size,\n",
    "    v_li_activ_fcts_model,\n",
    "    v_li_optimizers_model,\n",
    "    v_min_val_learning_rate_optimizer,\n",
    "    v_max_val_learning_rate_optimizer,\n",
    "    v_loss_fct_model,\n",
    "    v_metrics_model,\n",
    "    v_di_tuners,\n",
    "    v_key_tuner_class,\n",
    "    v_objective_metric_for_tuner_to_optimize,\n",
    "    v_mode,\n",
    "    v_max_trials,\n",
    "    v_executions_per_trial,\n",
    "    v_directory,\n",
    "    v_metric_for_tuner_search_hp_callback,\n",
    "    v_li_keys_tuners_optimizing_batch_size,\n",
    "    v_epochs_tuner_search,\n",
    "    v_top_best_models,\n",
    "    v_batch_size,\n",
    "    v_to_multiply_epoch_for_train_dur,\n",
    "    v_metric_to_monitor_best_epoch_callbacks,\n",
    "    v_epochs_best_trained_model_search,\n",
    "    v_overwrite=True,\n",
    "    v_patience_during_tuner_search=5,\n",
    "    v_verbose=2,\n",
    "    v_mode_callbacks=\"min\",\n",
    "    v_patience_best_epoch_callbacks=10,\n",
    "    v_pkl_filename_best_model = \"best_model\",\n",
    "    v_pkl_filename_best_retrained_model=\\\n",
    "    \"history_obj_best_retrained_model\"\n",
    "):\n",
    "\n",
    "    #di_rep=dict, key=id origin, value=[,... \n",
    "    #returned values of function fct_best_approaches_1 \n",
    "    #for the ith dataframe,....]\n",
    "    di_rep={}\n",
    "\n",
    "\n",
    "\n",
    "    #for each dataframe\n",
    "    for i in v_di_dataframes:\n",
    "        \n",
    "        #cur_dir=os.getcwd()\n",
    "        #os.chdir(v_folder_figures)\n",
    "        \n",
    "        #create the the folder\n",
    "        val_name_folder_figures_given_origin=v_folder_figures+\"/\"+v_folder_figures+\"_\"+str(i)\n",
    "        #print()\n",
    "        #print(\"val_name_folder_figures_given_origin\",val_name_folder_figures_given_origin)\n",
    "        #import sys\n",
    "        #sys.exit()\n",
    "        os.makedirs(val_name_folder_figures_given_origin,exist_ok = True)\n",
    "\n",
    "        #va_name_figure_folder_metric sera val_name_folder_origin\n",
    "        di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "        di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "        va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "        va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "        va_name_figure_folder_metric,va_name_figure_metric=fct_best_approaches_1(\n",
    "        v_dataframe=v_di_dataframes[i],\n",
    "        v_s_stride=v_s_stride,\n",
    "        v_b_size=v_b_size,\n",
    "        v_folder_figures=val_name_folder_figures_given_origin,\n",
    "        v_nb_past_seq_lengths=v_nb_past_seq_lengths,\n",
    "        v_nb_future_seq_lengths=v_nb_future_seq_lengths,\n",
    "        v_s_length_train_model=v_s_length_train_model,\n",
    "        v_s_length_target=v_s_length_target,\n",
    "        v_name_col_origin=v_name_col_origin,\n",
    "        v_li_name_col_to_copy=v_li_name_col_to_copy,\n",
    "        v_name_column_date=v_name_column_date,\n",
    "        v_name_col_2_sort=v_name_col_2_sort,\n",
    "        v_name_target_variable=v_name_target_variable,\n",
    "        v_li_cols_to_ignore=v_li_cols_to_ignore,\n",
    "        v_proportion_train_set=v_proportion_train_set,\n",
    "        v_proportion_val_set=v_proportion_val_set,\n",
    "        v_name_fig=v_name_fig,\n",
    "        v_shuffle_tr_s=v_shuffle_tr_s,\n",
    "        v_shuffle_tr_t=v_shuffle_tr_t,\n",
    "        v_shuffle_v_s=v_shuffle_v_s,\n",
    "        v_shuffle_v_t=v_shuffle_v_t,\n",
    "        v_shuffle_t_s=v_shuffle_t_s,\n",
    "        v_shuffle_t_t=v_shuffle_t_t,\n",
    "        v_di_hypermodels=v_di_hypermodels,\n",
    "        v_key_hypermodel_class=v_key_hypermodel_class,\n",
    "        v_min_nb_lay_model=v_min_nb_lay_model,\n",
    "        v_max_nb_lay_model=v_max_nb_lay_model,\n",
    "        v_min_nb_units_model=v_min_nb_units_model,\n",
    "        v_max_nb_units_model=v_max_nb_units_model,\n",
    "        v_min_value_dropout_rate_model=\\\n",
    "        v_min_value_dropout_rate_model,\n",
    "        v_max_value_dropout_rate_model=\\\n",
    "        v_max_value_dropout_rate_model,\n",
    "        v_min_value_recurrent_dropout_rate_model=\\\n",
    "        v_min_value_recurrent_dropout_rate_model,\n",
    "        v_max_value_recurrent_dropout_rate_model=\\\n",
    "        v_max_value_recurrent_dropout_rate_model,\n",
    "        v_min_nb_filters_conv1d=v_min_nb_filters_conv1d,\n",
    "        v_max_nb_filters_conv1d=v_max_nb_filters_conv1d,\n",
    "        v_min_nb_kernel_size_conv1d=v_min_nb_kernel_size_conv1d,\n",
    "        v_max_nb_kernel_size_conv1d=v_max_nb_kernel_size_conv1d,\n",
    "        v_step_nb_layers_model=v_step_nb_layers_model,\n",
    "        v_step_nb_units_model=v_step_nb_units_model,\n",
    "        v_step_dropout_rate_model=v_step_dropout_rate_model,\n",
    "        v_step_recurrent_dropout_rate_model=\\\n",
    "        v_step_recurrent_dropout_rate_model,\n",
    "        v_step_nb_kernel_size_conv1d=\\\n",
    "        v_step_nb_kernel_size_conv1d,\n",
    "        v_min_pool_size=v_min_pool_size,\n",
    "        v_max_pool_size=v_max_pool_size,\n",
    "        v_step_pool_size=v_step_pool_size,\n",
    "        v_li_activ_fcts_model=v_li_activ_fcts_model,\n",
    "        v_li_optimizers_model=v_li_optimizers_model,\n",
    "        v_min_val_learning_rate_optimizer=\\\n",
    "        v_min_val_learning_rate_optimizer,\n",
    "        v_max_val_learning_rate_optimizer=\\\n",
    "        v_max_val_learning_rate_optimizer,\n",
    "        v_loss_fct_model=v_loss_fct_model,\n",
    "        v_metrics_model=v_metrics_model,\n",
    "        v_di_tuners=v_di_tuners,\n",
    "        v_key_tuner_class=v_key_tuner_class,\n",
    "        v_objective_metric_for_tuner_to_optimize=\\\n",
    "        v_objective_metric_for_tuner_to_optimize,\n",
    "        v_mode=v_mode,\n",
    "        v_max_trials=v_max_trials,\n",
    "        v_executions_per_trial=v_executions_per_trial,\n",
    "        v_directory=v_directory,\n",
    "        v_metric_for_tuner_search_hp_callback=\\\n",
    "        v_metric_for_tuner_search_hp_callback,\n",
    "        v_li_keys_tuners_optimizing_batch_size=\\\n",
    "        v_li_keys_tuners_optimizing_batch_size,\n",
    "        v_epochs_tuner_search=v_epochs_tuner_search,\n",
    "        v_top_best_models=v_top_best_models,\n",
    "        v_batch_size=v_batch_size,\n",
    "        v_to_multiply_epoch_for_train_dur=\\\n",
    "        v_to_multiply_epoch_for_train_dur,\n",
    "        v_metric_to_monitor_best_epoch_callbacks=\\\n",
    "        v_metric_to_monitor_best_epoch_callbacks,\n",
    "        v_epochs_best_trained_model_search=\\\n",
    "        v_epochs_best_trained_model_search,\n",
    "        v_overwrite=True,\n",
    "        v_patience_during_tuner_search=5,\n",
    "        v_verbose=2,\n",
    "        v_mode_callbacks=\"min\",\n",
    "        v_patience_best_epoch_callbacks=10,\n",
    "        v_pkl_filename_best_model = \"best_model\",\n",
    "        v_pkl_filename_best_retrained_model=\\\n",
    "        \"history_obj_best_retrained_model\")\n",
    "\n",
    "        li=[di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "        di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "        va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "        va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "        va_name_figure_folder_metric,va_name_figure_metric]\n",
    "\n",
    "        di_rep[i]=li\n",
    "\n",
    "        #os.chdir(cur_dir)\n",
    "\n",
    "    return di_rep\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12af64c1-3a2f-4f52-a102-a45e93a99dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returning the best  model(s) approach\n",
    "\n",
    "#if all origins are examined together it returns a list\n",
    "#li_rep=di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "#di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "#va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "#va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "#va_name_figure_folder_metric,va_name_figure_metric]\n",
    "#otherwise if each origin  is examined separatly\n",
    "#it returns a dict, key=id origin\n",
    "#value= li_re^\n",
    "def fct_best_approaches(\n",
    "    v_distinct_origins,\n",
    "    v_dataframe,\n",
    "    v_s_stride,\n",
    "    v_b_size,\n",
    "    v_folder_figures,\n",
    "    v_nb_past_seq_lengths,\n",
    "    v_nb_future_seq_lengths,\n",
    "    v_s_length_train_model,\n",
    "    v_s_length_target,\n",
    "    v_name_col_origin,\n",
    "    v_li_name_col_to_copy,\n",
    "    v_name_column_date,\n",
    "    v_name_col_2_sort,\n",
    "    v_name_target_variable,\n",
    "    v_li_cols_to_ignore,\n",
    "    v_proportion_train_set,\n",
    "    v_proportion_val_set,\n",
    "    v_name_fig,\n",
    "    v_shuffle_tr_s,\n",
    "    v_shuffle_tr_t,\n",
    "    v_shuffle_v_s,\n",
    "    v_shuffle_v_t,\n",
    "    v_shuffle_t_s,\n",
    "    v_shuffle_t_t,\n",
    "    v_di_hypermodels,\n",
    "    v_key_hypermodel_class,\n",
    "    v_min_nb_lay_model,\n",
    "    v_max_nb_lay_model,\n",
    "    v_min_nb_units_model,\n",
    "    v_max_nb_units_model,\n",
    "    v_min_value_dropout_rate_model,\n",
    "    v_max_value_dropout_rate_model,\n",
    "    v_min_value_recurrent_dropout_rate_model,\n",
    "    v_max_value_recurrent_dropout_rate_model,\n",
    "    v_min_nb_filters_conv1d,\n",
    "    v_max_nb_filters_conv1d,\n",
    "    v_min_nb_kernel_size_conv1d,\n",
    "    v_max_nb_kernel_size_conv1d,\n",
    "    v_step_nb_layers_model,\n",
    "    v_step_nb_units_model,\n",
    "    v_step_dropout_rate_model,\n",
    "    v_step_recurrent_dropout_rate_model,\n",
    "    v_step_nb_kernel_size_conv1d,\n",
    "    v_min_pool_size,\n",
    "    v_max_pool_size,\n",
    "    v_step_pool_size,\n",
    "    v_li_activ_fcts_model,\n",
    "    v_li_optimizers_model,\n",
    "    v_min_val_learning_rate_optimizer,\n",
    "    v_max_val_learning_rate_optimizer,\n",
    "    v_loss_fct_model,\n",
    "    v_metrics_model,\n",
    "    v_di_tuners,\n",
    "    v_key_tuner_class,\n",
    "    v_objective_metric_for_tuner_to_optimize,\n",
    "    v_mode,\n",
    "    v_max_trials,\n",
    "    v_executions_per_trial,\n",
    "    v_directory,\n",
    "    v_metric_for_tuner_search_hp_callback,\n",
    "    v_li_keys_tuners_optimizing_batch_size,\n",
    "    v_epochs_tuner_search,\n",
    "    v_top_best_models,\n",
    "    v_batch_size,\n",
    "    v_to_multiply_epoch_for_train_dur,\n",
    "    v_metric_to_monitor_best_epoch_callbacks,\n",
    "    v_epochs_best_trained_model_search,\n",
    "    val_admissible_min_nb_observatios=80,\\\n",
    "    v_overwrite=True,\n",
    "    v_patience_during_tuner_search=5,\n",
    "    v_verbose=2,\n",
    "    v_mode_callbacks=\"min\",\n",
    "    v_patience_best_epoch_callbacks=10,\n",
    "    v_pkl_filename_best_model = \"best_model\",\n",
    "    v_pkl_filename_best_retrained_model=\\\n",
    "    \"history_obj_best_retrained_model\"\n",
    "):\n",
    "\n",
    "    #if we want to examine all  origins together\n",
    "    if v_distinct_origins==0:\n",
    "\n",
    "        #create the folder for the figures (plots)\n",
    "        #folder for the all the figure plots\n",
    "        #os.makedirs(v_folder_figures,exist_ok = True)\n",
    "\n",
    "        #li_rep=[di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "        #di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "        #va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "        #va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "        #va_name_figure_folder_metric,va_name_figure_metric]\n",
    "\n",
    "        li_rep=\\\n",
    "        fct_best_approaches_1(\n",
    "        v_dataframe=v_dataframe,\n",
    "        v_s_stride=v_s_stride,\n",
    "        v_b_size=v_b_size,\n",
    "        v_folder_figures=v_folder_figures,\n",
    "        v_nb_past_seq_lengths=v_nb_past_seq_lengths,\n",
    "        v_nb_future_seq_lengths=v_nb_future_seq_lengths,\n",
    "        v_s_length_train_model=v_s_length_train_model,\n",
    "        v_s_length_target=v_s_length_target,\n",
    "        v_name_col_origin=v_name_col_origin,\n",
    "        v_li_name_col_to_copy=v_li_name_col_to_copy,\n",
    "        v_name_column_date=v_name_column_date,\n",
    "        v_name_col_2_sort=v_name_col_2_sort,\n",
    "        v_name_target_variable=v_name_target_variable,\n",
    "        v_li_cols_to_ignore=v_li_cols_to_ignore,\n",
    "        v_proportion_train_set=v_proportion_train_set,\n",
    "        v_proportion_val_set=v_proportion_val_set,\n",
    "        v_name_fig=v_name_fig,\n",
    "        v_shuffle_tr_s=v_shuffle_tr_s,\n",
    "        v_shuffle_tr_t=v_shuffle_tr_t,\n",
    "        v_shuffle_v_s=v_shuffle_v_s,\n",
    "        v_shuffle_v_t=v_shuffle_v_t,\n",
    "        v_shuffle_t_s=v_shuffle_t_s,\n",
    "        v_shuffle_t_t=v_shuffle_t_t,\n",
    "        v_di_hypermodels=v_di_hypermodels,\n",
    "        v_key_hypermodel_class=v_key_hypermodel_class,\n",
    "        v_min_nb_lay_model=v_min_nb_lay_model,\n",
    "        v_max_nb_lay_model=v_max_nb_lay_model,\n",
    "        v_min_nb_units_model=v_min_nb_units_model,\n",
    "        v_max_nb_units_model=v_max_nb_units_model,\n",
    "        v_min_value_dropout_rate_model=\\\n",
    "        v_min_value_dropout_rate_model,\n",
    "        v_max_value_dropout_rate_model=\\\n",
    "        v_max_value_dropout_rate_model,\n",
    "        v_min_value_recurrent_dropout_rate_model=\\\n",
    "        v_min_value_recurrent_dropout_rate_model,\n",
    "        v_max_value_recurrent_dropout_rate_model=\\\n",
    "        v_max_value_recurrent_dropout_rate_model,\n",
    "        v_min_nb_filters_conv1d=v_min_nb_filters_conv1d,\n",
    "        v_max_nb_filters_conv1d=v_max_nb_filters_conv1d,\n",
    "        v_min_nb_kernel_size_conv1d=\\\n",
    "        v_min_nb_kernel_size_conv1d,\n",
    "        v_max_nb_kernel_size_conv1d=\\\n",
    "        v_max_nb_kernel_size_conv1d,\n",
    "        v_step_nb_layers_model=v_step_nb_layers_model,\n",
    "        v_step_nb_units_model=v_step_nb_units_model,\n",
    "        v_step_dropout_rate_model=v_step_dropout_rate_model,\n",
    "        v_step_recurrent_dropout_rate_model=\\\n",
    "        v_step_recurrent_dropout_rate_model,\n",
    "        v_step_nb_kernel_size_conv1d=\\\n",
    "        v_step_nb_kernel_size_conv1d,\n",
    "        v_min_pool_size=v_min_pool_size,\n",
    "        v_max_pool_size=v_max_pool_size,\n",
    "        v_step_pool_size=v_step_pool_size,\n",
    "        v_li_activ_fcts_model=v_li_activ_fcts_model,\n",
    "        v_li_optimizers_model=v_li_optimizers_model,\n",
    "        v_min_val_learning_rate_optimizer=\\\n",
    "        v_min_val_learning_rate_optimizer,\n",
    "        v_max_val_learning_rate_optimizer=\\\n",
    "        v_max_val_learning_rate_optimizer,\n",
    "        v_loss_fct_model=v_loss_fct_model,\n",
    "        v_metrics_model=v_metrics_model,\n",
    "        v_di_tuners=v_di_tuners,\n",
    "        v_key_tuner_class=v_key_tuner_class,\n",
    "        v_objective_metric_for_tuner_to_optimize=\\\n",
    "        v_objective_metric_for_tuner_to_optimize,\n",
    "        v_mode=v_mode,\n",
    "        v_max_trials=v_max_trials,\n",
    "        v_executions_per_trial=v_executions_per_trial,\n",
    "        v_directory=v_directory,\n",
    "        v_metric_for_tuner_search_hp_callback=\\\n",
    "        v_metric_for_tuner_search_hp_callback,\n",
    "        v_li_keys_tuners_optimizing_batch_size=\\\n",
    "        v_li_keys_tuners_optimizing_batch_size,\n",
    "        v_epochs_tuner_search=v_epochs_tuner_search,\n",
    "        v_top_best_models=v_top_best_models,\n",
    "        v_batch_size=v_batch_size,\n",
    "        v_to_multiply_epoch_for_train_dur=\\\n",
    "        v_to_multiply_epoch_for_train_dur,\n",
    "        v_metric_to_monitor_best_epoch_callbacks=\\\n",
    "        v_metric_to_monitor_best_epoch_callbacks,\n",
    "        v_epochs_best_trained_model_search=\\\n",
    "        v_epochs_best_trained_model_search,\n",
    "        v_overwrite=True,\n",
    "        v_patience_during_tuner_search=5,\n",
    "        v_verbose=2,\n",
    "        v_mode_callbacks=\"min\",\n",
    "        v_patience_best_epoch_callbacks=10,\n",
    "        v_pkl_filename_best_model = \"best_model\",\n",
    "        v_pkl_filename_best_retrained_model=\\\n",
    "        \"history_obj_best_retrained_model\")\n",
    "\n",
    "        return li_rep\n",
    "            \n",
    "    #if we want to examine each origin separatly \n",
    "    else:\n",
    "        #di_df_per_origin=dict, key=id  origin, value=df\n",
    "        di_df_per_origin=fct_create_di_dataframes_per_origin(\n",
    "        val_dataframe=v_dataframe,\n",
    "        val_admissible_min_nb_observatios=\\\n",
    "        val_admissible_min_nb_observatios,\n",
    "        val_name_col_origin=\"ORIGIN\",\n",
    "        val_name_col_1_sort='FL_DATE',\n",
    "        val_name_col_2_sort='DEP_TIME')\n",
    "\n",
    "        #di_rep=dict, key=id origin, \n",
    "        #value=[di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "        #di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "        #va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "        #va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "        #va_name_figure_folder_metric,va_name_figure_metric]\n",
    "        \n",
    "        di_rep=fct_best_approaches_2(\n",
    "        v_di_dataframes=di_df_per_origin,\n",
    "        v_s_stride=v_s_stride,\n",
    "        v_b_size=v_b_size,\n",
    "        v_folder_figures=v_folder_figures,\n",
    "        v_nb_past_seq_lengths=v_nb_past_seq_lengths,\n",
    "        v_nb_future_seq_lengths=v_nb_future_seq_lengths,\n",
    "        v_s_length_train_model=v_s_length_train_model,\n",
    "        v_s_length_target=v_s_length_target,\n",
    "        v_name_col_origin=v_name_col_origin,\n",
    "        v_li_name_col_to_copy=v_li_name_col_to_copy,\n",
    "        v_name_column_date=v_name_column_date,\n",
    "        v_name_col_2_sort=v_name_col_2_sort,\n",
    "        v_name_target_variable=v_name_target_variable,\n",
    "        v_li_cols_to_ignore=v_li_cols_to_ignore,\n",
    "        v_proportion_train_set=v_proportion_train_set,\n",
    "        v_proportion_val_set=v_proportion_val_set,\n",
    "        v_name_fig=v_name_fig,\n",
    "        v_shuffle_tr_s=v_shuffle_tr_s,\n",
    "        v_shuffle_tr_t=v_shuffle_tr_t,\n",
    "        v_shuffle_v_s=v_shuffle_v_s,\n",
    "        v_shuffle_v_t=v_shuffle_v_t,\n",
    "        v_shuffle_t_s=v_shuffle_t_s,\n",
    "        v_shuffle_t_t=v_shuffle_t_t,\n",
    "        v_di_hypermodels=v_di_hypermodels,\n",
    "        v_key_hypermodel_class=v_key_hypermodel_class,\n",
    "        v_min_nb_lay_model=v_min_nb_lay_model,\n",
    "        v_max_nb_lay_model=v_max_nb_lay_model,\n",
    "        v_min_nb_units_model=v_min_nb_units_model,\n",
    "        v_max_nb_units_model=v_max_nb_units_model,\n",
    "        v_min_value_dropout_rate_model=v_min_value_dropout_rate_model,\n",
    "        v_max_value_dropout_rate_model=v_max_value_dropout_rate_model,\n",
    "        v_min_value_recurrent_dropout_rate_model=v_min_value_recurrent_dropout_rate_model,\n",
    "        v_max_value_recurrent_dropout_rate_model=v_max_value_recurrent_dropout_rate_model,\n",
    "        v_min_nb_filters_conv1d=v_min_nb_filters_conv1d,\n",
    "        v_max_nb_filters_conv1d=v_max_nb_filters_conv1d,\n",
    "        v_min_nb_kernel_size_conv1d=v_min_nb_kernel_size_conv1d,\n",
    "        v_max_nb_kernel_size_conv1d=v_max_nb_kernel_size_conv1d,\n",
    "        v_step_nb_layers_model=v_step_nb_layers_model,\n",
    "        v_step_nb_units_model=v_step_nb_units_model,\n",
    "        v_step_dropout_rate_model=v_step_dropout_rate_model,\n",
    "        v_step_recurrent_dropout_rate_model=v_step_recurrent_dropout_rate_model,\n",
    "        v_step_nb_kernel_size_conv1d=v_step_nb_kernel_size_conv1d,\n",
    "        v_min_pool_size=v_min_pool_size,\n",
    "        v_max_pool_size=v_max_pool_size,\n",
    "        v_step_pool_size=v_step_pool_size,\n",
    "        v_li_activ_fcts_model=v_li_activ_fcts_model,\n",
    "        v_li_optimizers_model=v_li_optimizers_model,\n",
    "        v_min_val_learning_rate_optimizer=v_min_val_learning_rate_optimizer,\n",
    "        v_max_val_learning_rate_optimizer=v_max_val_learning_rate_optimizer,\n",
    "        v_loss_fct_model=v_loss_fct_model,\n",
    "        v_metrics_model=v_metrics_model,\n",
    "        v_di_tuners=v_di_tuners,\n",
    "        v_key_tuner_class=v_key_tuner_class,\n",
    "        v_objective_metric_for_tuner_to_optimize=v_objective_metric_for_tuner_to_optimize,\n",
    "        v_mode=v_mode,\n",
    "        v_max_trials=v_max_trials,\n",
    "        v_executions_per_trial=v_executions_per_trial,\n",
    "        v_directory=v_directory,\n",
    "        v_metric_for_tuner_search_hp_callback=v_metric_for_tuner_search_hp_callback,\n",
    "        v_li_keys_tuners_optimizing_batch_size=v_li_keys_tuners_optimizing_batch_size,\n",
    "        v_epochs_tuner_search=v_epochs_tuner_search,\n",
    "        v_top_best_models=v_top_best_models,\n",
    "        v_batch_size=v_batch_size,\n",
    "        v_to_multiply_epoch_for_train_dur=v_to_multiply_epoch_for_train_dur,\n",
    "        v_metric_to_monitor_best_epoch_callbacks=v_metric_to_monitor_best_epoch_callbacks,\n",
    "        v_epochs_best_trained_model_search=v_epochs_best_trained_model_search,\n",
    "        v_overwrite=True,\n",
    "        v_patience_during_tuner_search=5,\n",
    "        v_verbose=2,\n",
    "        v_mode_callbacks=\"min\",\n",
    "        v_patience_best_epoch_callbacks=10,\n",
    "        v_pkl_filename_best_model = \"best_model\",\n",
    "        v_pkl_filename_best_retrained_model=\\\n",
    "        \"history_obj_best_retrained_model\")\n",
    "\n",
    "        return di_rep\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f09271-e5c8-43d5-9328-18fb496cf42f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821aac5-c5c6-43b2-8bb5-d5e71d739634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7d07c98-c3c4-47ce-976f-53e14655cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct making inferences from the first best model registered in a list\n",
    "#with the best models\n",
    "\n",
    "#it returns a dict, \n",
    "#key=id best model, value=list destandardized  predictions \n",
    "\n",
    "def fct_create_di_destand_predictions(\n",
    "    val_di_best_models,\n",
    "    val_test_dataset,\n",
    "    val_mean_value_train_dataset,\n",
    "    val_std_train_dataset):\n",
    "        \n",
    "        #di_destand_predicts = dict\n",
    "        #key=id best  model, value=list destandardized  predictions \n",
    "        di_destand_predicts={}\n",
    "        \n",
    "        \n",
    "        #for each  best model we make predictions\n",
    "        for i in val_di_best_models:\n",
    "             \n",
    "            preds=val_di_best_models[i].predict(val_test_dataset)\n",
    "            #print(\"type(preds)\",type(preds))\n",
    "            #print(\"preds.shape\",preds.shape)\n",
    "        \n",
    "            li=\\\n",
    "            [x * val_std_train_dataset+val_mean_value_train_dataset for x in preds]\n",
    "\n",
    "            #print(\"len(li)\",len(li))\n",
    "            #print(\"len(li[0])\",len(li[0]))\n",
    "            \n",
    "        \n",
    "            #li_destandardized_preds=np.array(li).reshape(-1)\n",
    "\n",
    "            #print(\"li_destandardized_preds.shape\",li_destandardized_preds.shape)\n",
    "            \n",
    "            di_destand_predicts[i]=li\n",
    "                   \n",
    "        return di_destand_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889adad9-0966-48c6-9bea-95a0d1858ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58ad134f-c2e5-433b-bd3b-8388ef976f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fct_plot_inferences_single_set(\\\n",
    "    val_past_values,\\\n",
    "    val_true_future_values,\\\n",
    "    val_prediction,\\\n",
    "    val_name_folder_plots,\\\n",
    "    val_name_figure_inferences,\\\n",
    "    val_title=\"Inferences versus True Future Values\"):\n",
    "\n",
    "    sns.set(palette=\"hot\")\n",
    "    plt.rcParams['lines.markersize'] = 10\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    past_t_steps=list(range(-len(val_past_values), 0))\n",
    "    \n",
    "    future_t_steps = np.arange(len(val_true_future_values))\n",
    "\n",
    "    #print(\"future_t_steps\",future_t_steps)\n",
    "    \n",
    "    #plot the past values of arrival delay used for  learning\n",
    "    plt.plot(past_t_steps, np.array(val_past_values[:, 7]), label='Past Values',color=\"green\")\n",
    "\n",
    "    #plot the true future values\n",
    "    plt.plot(future_t_steps, np.array(val_true_future_values), \n",
    "           label='True Future Values',color=\"darkmagenta\")\n",
    "        \n",
    "    #plot predictions\n",
    "    plt.plot(future_t_steps, np.array(val_prediction),\n",
    "    label='Predicted Future Values',color=\"gold\")\n",
    "        \n",
    "    \n",
    "        \n",
    "    plt.title(val_title,c='mediumblue')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(val_name_folder_plots+\"/\"+\\\n",
    "                val_name_figure_inferences+str(\".png\"))\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6fdcc-6265-4ad0-aa6d-a6a9cc9128dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "656ed290-60d5-4241-b7ee-dd834f5bfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fct_plot_inferences(\\\n",
    "    val_nb_takes,\\\n",
    "    val_test_set,\n",
    "    val_di_destandardized_predictions,\\\n",
    "    val_name_folder_with_plots,\\\n",
    "    val_name_file_plot_inferences,\\\n",
    "    val_title=\"Inferences versus True Future Values\"):\n",
    "\n",
    "        for m in val_di_destandardized_predictions:\n",
    "            ind=0\n",
    "            for  i, j in val_test_set.take(val_nb_takes):\n",
    "\n",
    "                ind+=1\n",
    "\n",
    "                v_name_file_plot_inferences=\\\n",
    "                val_name_file_plot_inferences+\"_\"+\"best_model_\"+str(m)+\"_\"+str(ind)\n",
    "\n",
    "                fct_plot_inferences_single_set(\\\n",
    "                val_past_values=i[0],\\\n",
    "                val_true_future_values=j[0],\\\n",
    "                val_prediction=val_di_destandardized_predictions[m][0],\\\n",
    "                val_name_folder_plots=val_name_folder_with_plots,\\\n",
    "                val_name_figure_inferences=\\\n",
    "                v_name_file_plot_inferences,\\\n",
    "                val_title=val_title)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2a72e-ee64-484f-8bec-c1fe471040b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0f9cfbf-5974-4a5f-b2d0-5cdfc1c7b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which plots the predicted values versus\n",
    "#the true values for a single list of predictions\n",
    "\n",
    "\n",
    "def fct_plot_train_metrics_retrained_best_model(\n",
    "    val_di_hist_retrained_best_model,\n",
    "    val_li_colors,\n",
    "    val_li_markers,\n",
    "    val_name_figure_folder_metric,\n",
    "    val_name_figure,\n",
    "    val_name_figure_loss=\"best_retrained_model\",\n",
    "    val_title=\\\n",
    "    \"Train Set Metrics- Retrained Best Model,\",\n",
    "    val_y_label=\"Metric Value\",\\\n",
    "    val_loc=\"best\"):\n",
    "    \n",
    "    sns.set(palette=\"hot\")\n",
    "    plt.rcParams['lines.markersize'] = 10\n",
    "    \n",
    "    #with plt.rc_context({'axes.edgecolor':'blue',\\\n",
    "    # 'xtick.color':'dodgerblue', 'ytick.color':'dodgerblue',\n",
    "    # 'figure.facecolor':'white'}):\n",
    "        \n",
    "    plt.figure(figsize=(10,12))\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"bottom\"].set_color(\"green\")\n",
    "    ax.spines[\"top\"].set_color(\"green\")\n",
    "    ax.spines[\"left\"].set_color(\"green\")\n",
    "    ax.spines[\"right\"].set_color(\"green\")\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", colors=\"forestgreen\")     \n",
    "    ax.tick_params(axis=\"y\", colors=\"forestgreen\")\n",
    "    \n",
    "    #sns.set(style=\"darkgrid\")\n",
    "\n",
    "    #sns.set_style(\"darkgrid\", \\\n",
    "    #{\"grid.color\": \".3\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "    #res=sns.set(\\\n",
    "    #rc={'axes.facecolor':'lightblue', \\\n",
    "    #'figure.facecolor':'cornflowerblue'})\n",
    "    #**************\n",
    "    \n",
    "    \n",
    "    #di_hist_retrained_best_model\n",
    "    #di_hist_retrained_best_model=dictionary,\n",
    "    #key=id best model (starting with 1)\n",
    "    #value=history retrained best model\n",
    "    #this history cotnains values for both train and\n",
    "    #validation sets\n",
    "    for i in val_di_hist_retrained_best_model:\n",
    "        \n",
    "        li_labels=[]\n",
    "        \n",
    "        ind=0\n",
    "        \n",
    "        #print()\n",
    "        #print(\"ind in boucle i\",ind)\n",
    "    \n",
    "        #print(\"Evaluation Best retrained model :\", i)\n",
    "        #for each metric\n",
    "        for j in val_di_hist_retrained_best_model[i].history:\n",
    "            \n",
    "            #print(\"metric\",j)\n",
    "            \n",
    "            \n",
    "            if j!=\"loss\" and j!=\"val_loss\":\n",
    "                \n",
    "                #print(\"values per epoch:\",val_di_hist_retrained_best_model[i].history[j])\n",
    "            \n",
    "                li_vals_metric_j=val_di_hist_retrained_best_model[i].history[j]\n",
    "                \n",
    "                a=len(li_vals_metric_j)\n",
    "            \n",
    "                li_id_epochs=list(range(1,a+1))\n",
    "            \n",
    "                #print(\"here\",val_di_hist_retrained_best_model[i].history[j])\n",
    "                graph=sns.lineplot(x=li_id_epochs,\\\n",
    "                y = val_di_hist_retrained_best_model[i].history[j],label=str(j),\\\n",
    "                color=val_li_colors[ind],marker=val_li_markers[ind])\n",
    "            \n",
    "                #print(\"val_li_colors[ind]\",val_li_colors[ind])\n",
    "                \n",
    "                ind+=1\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        plt.xlabel(\"Epoch ID\",c='mediumblue')\n",
    "        plt.ylabel(val_y_label,c='mediumblue')\n",
    "        #plt.xticks(li_id_epochs)\n",
    "        \n",
    "        v_title=val_title+\" \"+str(i)\n",
    "        plt.title(v_title,c='mediumblue')\n",
    "        \n",
    "    \n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        \n",
    "        plt.grid(True)\n",
    "        plt.savefig(val_name_figure_folder_metric+\"/\"+\\\n",
    "                    val_name_figure+\"_\"+str(i)+str(\".png\"))\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    \n",
    "    #print(\"val_di_hist_retrained_best_model[i].history.keys()\",val_di_hist_retrained_best_model[i].history.keys())\n",
    "    #we plot the loss\n",
    "    \n",
    "    for i in val_di_hist_retrained_best_model:\n",
    "        #print(\"we plot loss or val_loss, model:\",i)\n",
    "        \n",
    "        #print(\"here1\",ind,ind+1,len(val_li_markers))\n",
    "        \n",
    "        if \"loss\" in val_di_hist_retrained_best_model[i].history:\n",
    "        \n",
    "            li_vals_metric_j=val_di_hist_retrained_best_model[i].history[\"loss\"]\n",
    "\n",
    "\n",
    "            a=len(li_vals_metric_j)\n",
    "            \n",
    "            li_id_epochs=list(range(1,a+1))\n",
    "\n",
    "    \n",
    "            #print(\"here\",val_di_hist_retrained_best_model[i].history[j])\n",
    "            graph=sns.lineplot(x=li_id_epochs,\\\n",
    "            y = val_di_hist_retrained_best_model[i].history[\"loss\"],\\\n",
    "            label=\"loss\",\\\n",
    "            color=val_li_colors[ind],marker=val_li_markers[ind])\n",
    "        \n",
    "            #if we have loss metrics on  validation set\n",
    "            if \"val_loss\" in val_di_hist_retrained_best_model[i].history:\n",
    "            \n",
    "                #print(\"here\",val_di_hist_retrained_best_model[i].history[j])\n",
    "            \n",
    "                graph=sns.lineplot(x=li_id_epochs,\\\n",
    "                y = val_di_hist_retrained_best_model[i].history[\"val_loss\"],\\\n",
    "                label=\"val_loss\",\\\n",
    "                color=val_li_colors[ind+1],marker=val_li_markers[ind+1])\n",
    "            \n",
    "                v_title=val_title+\" \"+str(i)+\" \"+\": Loss\"+\" \"+\": Loss, Val_Loss\"\n",
    "                \n",
    "                v_name_save_fig=val_name_figure_folder_metric+\"/\"+\\\n",
    "                \"fig_plot_metric_\"+\"loss_during_search_\"+\\\n",
    "                val_name_figure_loss+\"_\"+str(i)+str(\".png\")\n",
    "            else:\n",
    "                v_title=val_title+\" \"+str(i)+\" \"+\": Loss\"\n",
    "                \n",
    "            v_name_save_fig=val_name_figure_folder_metric+\"/\"+\\\n",
    "            val_name_figure+\"_\"+\\\n",
    "            val_name_figure_loss+\"_\"+str(i)+str(\".png\")\n",
    "                \n",
    "                \n",
    "    \n",
    "        plt.xlabel(\"Epoch ID\",c='mediumblue')\n",
    "    \n",
    "        plt.ylabel(val_y_label,c='mediumblue')\n",
    "        #plt.xticks(li_id_epochs)\n",
    "\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        \n",
    "        plt.title(v_title,c='mediumblue')\n",
    "        \n",
    "        plt.grid(True)\n",
    "        plt.savefig(v_name_save_fig)\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cd433-1e3f-46e4-894b-d6b9d2492ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3cc2c0f-e281-4bc7-a3fa-82e9adb2857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_di_results_model_eval_test_set=dictionary,\n",
    "#key=id best model (starting with 1)\n",
    "#value=dict, key=id metric, value=value metric\n",
    "    \n",
    "\n",
    "def fct_arrange_metrics_test_set_per_model(val_di_results_model_eval_test_set):\n",
    "    \n",
    "    #key=id metric, vaulue=[...,value metric model i,...]\n",
    "    di={}\n",
    "    \n",
    "    #for each model\n",
    "    for i in val_di_results_model_eval_test_set:\n",
    "        #for each metric of the model\n",
    "        for j in val_di_results_model_eval_test_set[i]:\n",
    "            #if the metric is in the dictionary\n",
    "            \n",
    "            if j in di:\n",
    "                di[j].append(val_di_results_model_eval_test_set[i][j])\n",
    "            \n",
    "            #if the metric is not in the diction\n",
    "            else:\n",
    "                di[j]=[val_di_results_model_eval_test_set[i][j]]\n",
    "                \n",
    "    return di\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315b798-6a3a-4fe8-bcff-07332f962137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d965b56a-312d-41f3-95fe-9ce024c2a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_di=dict, \n",
    "#value=dict, key=id metric, value=[...,value metric ith. model,....]\n",
    "def fct_plot_metrics_test_set_per_model(\\\n",
    "val_di,\n",
    "val_name_figure_folder,\\\n",
    "val_name_figure,\\\n",
    "val_li_colors,\\\n",
    "val_li_markers,\\\n",
    "val_x_label=\"Best Model ID\",\\\n",
    "val_y_label=\"Value Metric\",\\\n",
    "val_title=\"Metrics Test Set Per Each Best Model\"):\n",
    "    \n",
    "    sns.set(palette=\"hot\")\n",
    "    plt.rcParams['lines.markersize'] = 10\n",
    "    \n",
    "    #with plt.rc_context({'axes.edgecolor':'blue',\\\n",
    "    # 'xtick.color':'dodgerblue', 'ytick.color':'dodgerblue',\n",
    "    # 'figure.facecolor':'white'}):\n",
    "        \n",
    "    plt.figure(figsize=(10,12))\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"bottom\"].set_color(\"green\")\n",
    "    ax.spines[\"top\"].set_color(\"green\")\n",
    "    ax.spines[\"left\"].set_color(\"green\")\n",
    "    ax.spines[\"right\"].set_color(\"green\")\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", colors=\"forestgreen\")     \n",
    "    ax.tick_params(axis=\"y\", colors=\"forestgreen\")\n",
    "\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    #sns.set(style=\"darkgrid\")\n",
    "\n",
    "    #sns.set_style(\"darkgrid\", \\\n",
    "    #{\"grid.color\": \".3\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "    #res=sns.set(\\\n",
    "    #rc={'axes.facecolor':'lightblue', \\\n",
    "    #'figure.facecolor':'cornflowerblue'})\n",
    "    #**************\n",
    "    \n",
    "    #we plot each metric in a different figure\n",
    "    \n",
    "    ind=0\n",
    "    \n",
    "    #for each metric\n",
    "    for i in val_di:\n",
    "        \n",
    "        a=len(val_di[i])\n",
    "            \n",
    "        li_id_models=list(range(1,a+1))\n",
    "            \n",
    "        #print(\"here\",val_di_hist_retrained_best_model[i].history[j])\n",
    "        graph=sns.lineplot(x=li_id_models,\\\n",
    "        y = val_di[i],label=str(i),\\\n",
    "        color=val_li_colors[ind],marker=val_li_markers[ind])\n",
    "        \n",
    "        ind+=1\n",
    "        \n",
    "        plt.xlabel(val_x_label,c='mediumblue')\n",
    "        plt.ylabel(val_y_label,c='mediumblue')\n",
    "        #plt.xticks(li_id_epochs)\n",
    "        \n",
    "        \n",
    "        plt.title(val_title,c='mediumblue')\n",
    "        \n",
    "    \n",
    "    \n",
    "        plt.grid(True)\n",
    "        plt.savefig(val_name_figure_folder+\"/\"+\\\n",
    "                    val_name_figure+\"_metric_\"+str(i)+str(\".png\"))\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fbaa9a-b7dd-43e9-a753-c7574728dafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f8c4ab5-551b-4a54-a551-57832596a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which plots the graph for a dict of models\n",
    "#val_di=dict,key=id  model, value=model\n",
    "#val_name_file_plot_graph =the name of the file \n",
    "#without the extention for the type of the file\n",
    "def fct_plot_graph_di_models(\\\n",
    "    val_di,\\\n",
    "    val_name_folder_figures,    \n",
    "    val_name_file_plot_graph):\n",
    "\n",
    "        #for each model\n",
    "        for i in val_di:\n",
    "            #we  create the name of the file\n",
    "            file_name=val_name_file_plot_graph+\"_\"+str(i)+\".png\"\n",
    "            \n",
    "            plot_model(val_di[i], \n",
    "                       to_file=val_name_folder_figures+\"/\"+file_name,\n",
    "            show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ca8a3-5c99-4e24-bd6f-5efb8ff0dd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66c00ef1-721c-4b81-a0e2-677ee2f5c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#va_loc=\"best\"\n",
    "\n",
    "#va_li_responses=list\n",
    "#[di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "# di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    " # va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "# va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "# va_name_figure_folder_metric,va_name_figure_metric] \n",
    "\n",
    "def fct_analyze_results_1(\n",
    "    va_li_responses,\n",
    "    va_name_figure_metric_for_predicts,\n",
    "    va_mae_test_set,\n",
    "    va_rmse_test_set,\n",
    "    va_li_colors,\n",
    "    va_li_markers,\n",
    "    va_name_figure_find_best_epoch,\n",
    "    va_name_figure_loss_best_epoch,\n",
    "    va_title_best_epoch,\n",
    "    va_folder_figures,\n",
    "    va_name_figure_best_traind_model,\n",
    "    va_name_figure_loss_best_trained_model,\\\n",
    "    va_name_figure_plots_test_set,\\\n",
    "    va_name_file_plot_graph,\n",
    "    va_nb_takes_plot_inferences,\\\n",
    "    val_x_label=\"Best Model ID\",\\\n",
    "    val_y_label=\"Value Metric\",\\\n",
    "    val_title=\"Metrics Test Set Per Each Best Model\",\n",
    "    val_title_best_trained_model=\\\n",
    "    \"Train Set Metrics - Retrained Best Model\",\n",
    "    va_loc=\"best\",\\\n",
    "    val_title_inferences_plot=\"Inferences versus True Future Values\"\n",
    "):\n",
    "    \n",
    "    #we plot the graph for each best model\n",
    "    fct_plot_graph_di_models(\\\n",
    "    val_di=va_li_responses[0],\\\n",
    "    #val_name_folder_figures=va_folder_figures,\n",
    "    val_name_folder_figures=va_li_responses[9],\n",
    "    val_name_file_plot_graph=va_name_file_plot_graph)\n",
    "\n",
    "\n",
    "    #va_li_responses=list\n",
    "    #[di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "    # di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "    # va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "    # va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "    # va_name_figure_folder_metric,va_name_figure_metric]    \n",
    "    \n",
    "    #di_destand_predicts=dict\n",
    "    #key=id best model (starting with 1)\n",
    "    #value= list destandardized predictions \n",
    "    di_destand_predicts=fct_create_di_destand_predictions(\n",
    "    val_di_best_models=va_li_responses[0],\n",
    "    val_test_dataset=va_li_responses[4],\n",
    "    val_mean_value_train_dataset=va_li_responses[5],\n",
    "    val_std_train_dataset=va_li_responses[6])\n",
    "    \n",
    "    \n",
    "    #print(\"di_destand_predicts\",di_destand_predicts)\n",
    "    \n",
    "    fct_plot_inferences(\\\n",
    "    val_nb_takes=va_nb_takes_plot_inferences,\\\n",
    "    val_test_set=va_li_responses[4],\n",
    "    val_di_destandardized_predictions=di_destand_predicts,\\\n",
    "    #val_name_folder_with_plots=va_folder_figures,\\\n",
    "    val_name_folder_with_plots=va_li_responses[9],\n",
    "    val_name_file_plot_inferences=va_name_figure_metric_for_predicts,\\\n",
    "    val_title=val_title_inferences_plot)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #we plot the metrics for finding the best number of epochs \n",
    "    #for the best models\n",
    "    fct_plot_train_metrics_retrained_best_model(\n",
    "    val_di_hist_retrained_best_model=va_li_responses[1],\n",
    "    val_li_colors=va_li_colors,\n",
    "    val_li_markers=va_li_markers,\n",
    "    #val_name_figure_folder_metric=va_folder_figures,\n",
    "    val_name_figure_folder_metric=va_li_responses[9],\n",
    "    val_name_figure=va_name_figure_find_best_epoch,\n",
    "    val_name_figure_loss=va_name_figure_loss_best_epoch,\n",
    "    val_title=va_title_best_epoch,\n",
    "    val_y_label=\"Metric Value\",\\\n",
    "    val_loc=\"best\")\n",
    "    \n",
    "    #we  plot the metrics for the best trained model(s)\n",
    "    #trained for the best number of epochs\n",
    "    \n",
    "    fct_plot_train_metrics_retrained_best_model(\n",
    "    val_di_hist_retrained_best_model=va_li_responses[2],\n",
    "    val_li_colors=va_li_colors,\n",
    "    val_li_markers=va_li_markers,\n",
    "    #val_name_figure_folder_metric=va_folder_figures,\n",
    "    val_name_figure_folder_metric=va_li_responses[9],\n",
    "    val_name_figure=va_name_figure_best_traind_model,\n",
    "    val_name_figure_loss=va_name_figure_loss_best_trained_model,\\\n",
    "    val_title=\\\n",
    "    \"Train Set Metrics - Retrained Best Model\",\n",
    "    val_y_label=\"Metric Value\",\\\n",
    "    val_loc=\"best\")\n",
    "\n",
    "    #va_li_responses=list\n",
    "    #[di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "    # di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "    # va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "    # va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "    # va_name_figure_folder_metric,va_name_figure_metric]   \n",
    "    \n",
    "    #we create the dictionary, \n",
    "    #key=metric for evaluating best model(s) on the test set\n",
    "    #value=[... metric test set  using the ith best model,....]\n",
    "    \n",
    "    di_metrics_test_set=fct_arrange_metrics_test_set_per_model(\\\n",
    "    val_di_results_model_eval_test_set=va_li_responses[3])\n",
    "    \n",
    "    \n",
    "    #we plot the metrics when evaluation the best model(s) on the test set\n",
    "    fct_plot_metrics_test_set_per_model(\\\n",
    "    val_di=di_metrics_test_set,\n",
    "    #val_name_figure_folder=va_folder_figures,\\\n",
    "    val_name_figure_folder=va_li_responses[9],\n",
    "    val_name_figure=va_name_figure_plots_test_set,\\\n",
    "    val_li_colors=va_li_colors,\\\n",
    "    val_li_markers=va_li_markers,\\\n",
    "    val_x_label=\"Best Model ID\",\\\n",
    "    val_y_label=\"Value Metric\",\\\n",
    "    val_title=\"Metrics Test Set Per Each Best Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809d851-4fbb-41b0-a832-56ad40935a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#va_loc=\"best\"\n",
    "\n",
    "#va_di_respones=dict, key=id origin\n",
    "#value=#va_li_responses=list\n",
    "#[di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "# di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "#va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "# va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "# va_name_figure_folder_metric,va_name_figure_metric] \n",
    "\n",
    "def fct_analyze_results_2(\n",
    "    va_di_responses,\n",
    "    va_folder_figures,\n",
    "    va_mae_test_set,\n",
    "    va_rmse_test_set,\n",
    "    va_li_colors,\n",
    "    va_li_markers,\n",
    "    va_name_figure_metric_for_predicts,\n",
    "    va_name_figure_find_best_epoch,\n",
    "    va_name_figure_loss_best_epoch,\n",
    "    va_title_best_epoch,\n",
    "    va_name_figure_best_traind_model,\n",
    "    va_name_figure_loss_best_trained_model,\\\n",
    "    va_name_figure_plots_test_set,\\\n",
    "    va_name_file_plot_graph,\n",
    "    va_nb_takes_plot_inferences,\\\n",
    "    val_x_label=\"Best Model ID\",\\\n",
    "    val_y_label=\"Value Metric\",\\\n",
    "    val_title=\"Metrics Test Set Per Each Best Model\",\n",
    "    val_title_best_trained_model=\\\n",
    "    \"Train Set Metrics - Retrained Best Model\",\n",
    "    va_loc=\"best\",\\\n",
    "    val_title_inferences_plot=\"Inferences versus True Future Values\"\n",
    "):\n",
    "    #va_di_responses=dict, key=id origin\n",
    "    #[di_best_trained_models,di_hist_when_search_best_epoch,\\\n",
    "    # di_hist_retrained_best_model,di_results_model_eval_test_set,\\\n",
    "    # va_test_dataset,va_mean_value_train_dataset,va_std_train_dataset,\\\n",
    "    # va_id_first_future_observation,va_li_true_vals_test_set,\\\n",
    "    # va_name_figure_folder_metric,va_name_figure_metric]\n",
    "\n",
    "    #we analyze each origin\n",
    "    for i in va_di_responses:\n",
    "\n",
    "        print(\"analyze  origin: \", i)\n",
    "        #print(\"va_folder_figures=\",va_folder_figures)\n",
    "        #print(\"va_folder_figures=\",va_di_responses[i][9])\n",
    "        #print()\n",
    "        \n",
    "        #va_folder_figures=va_folder_figures+\"/\"+va_folder_figures+\"_\"+str(i)\n",
    "\n",
    "        #v_folder_figures+\"/\"+v_folder_figures+\"_\"+str(i)\n",
    "\n",
    "        fct_analyze_results_1(\n",
    "        va_li_responses=va_di_responses[i],\n",
    "        va_name_figure_metric_for_predicts=\\\n",
    "        va_name_figure_metric_for_predicts,\n",
    "        va_mae_test_set=va_mae_test_set,\n",
    "        va_rmse_test_set=va_rmse_test_set,\n",
    "        va_li_colors= va_li_colors,\n",
    "        va_li_markers=va_li_markers,\n",
    "        va_name_figure_find_best_epoch=\\\n",
    "        va_name_figure_find_best_epoch,\n",
    "        va_name_figure_loss_best_epoch=\\\n",
    "        va_name_figure_loss_best_epoch,\n",
    "        va_title_best_epoch=va_title_best_epoch,\n",
    "        #va_folder_figures=va_folder_figures,\n",
    "        va_folder_figures=va_di_responses[i][9],\n",
    "        va_name_figure_best_traind_model=\\\n",
    "        va_name_figure_best_traind_model,\n",
    "        va_name_figure_loss_best_trained_model=\\\n",
    "        va_name_figure_loss_best_trained_model,\n",
    "        va_name_figure_plots_test_set=\\\n",
    "        va_name_figure_plots_test_set,\n",
    "        va_name_file_plot_graph=\\\n",
    "        va_name_file_plot_graph,\n",
    "        va_nb_takes_plot_inferences=\\\n",
    "        va_nb_takes_plot_inferences,\n",
    "        val_x_label=\"Best Model ID\",\\\n",
    "        val_y_label=\"Value Metric\",\\\n",
    "        val_title=\"Metrics Test Set Per Each Best Model\",\n",
    "        val_title_best_trained_model=\\\n",
    "        \"Train Set Metrics - Retrained Best Model\",\n",
    "        va_loc=\"best\",\\\n",
    "        val_title_inferences_plot=\"Inferences versus True Future Values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e21abe1-4fc5-492c-b7d5-b11c18adb1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#va_di_respones=None if we wish o examine all the origins together\n",
    "\n",
    "def fct_analyze_results(\\\n",
    "    va_distinct_origins,\\\n",
    "    va_li_or_di_responses,\\\n",
    "    va_name_figure_metric_for_predicts,\n",
    "    va_mae_test_set,\n",
    "    va_rmse_test_set,\n",
    "    va_li_colors,\n",
    "    va_li_markers,\n",
    "    va_name_figure_find_best_epoch,\n",
    "    va_name_figure_loss_best_epoch,\n",
    "    va_title_best_epoch,\n",
    "    va_folder_figures,\n",
    "    va_name_figure_best_traind_model,\n",
    "    va_name_figure_loss_best_trained_model,\\\n",
    "    va_name_figure_plots_test_set,\\\n",
    "    va_name_file_plot_graph,\n",
    "    va_nb_takes_plot_inferences,\\\n",
    "    val_x_label=\"Best Model ID\",\\\n",
    "    val_y_label=\"Value Metric\",\\\n",
    "    val_title=\"Metrics Test Set Per Each Best Model\",\n",
    "    val_title_best_trained_model=\\\n",
    "    \"Train Set Metrics - Retrained Best Model\",\n",
    "    va_loc=\"best\",\\\n",
    "    val_title_inferences_plot=\"Inferences versus True Future Values\"):\n",
    "\n",
    "        #if all origins will be  examined at the same  time\n",
    "        if va_distinct_origins==0:\n",
    "\n",
    "            fct_analyze_results_1(\n",
    "            va_li_responses=va_li_or_di_responses,\n",
    "            va_name_figure_metric_for_predicts=\\\n",
    "            va_name_figure_metric_for_predicts,\n",
    "            va_mae_test_set=va_mae_test_set,\n",
    "            va_rmse_test_set=va_rmse_test_set,\n",
    "            va_li_colors= va_li_colors,\n",
    "            va_li_markers=va_li_markers,\n",
    "            va_name_figure_find_best_epoch=\\\n",
    "            va_name_figure_find_best_epoch,\n",
    "            va_name_figure_loss_best_epoch=\\\n",
    "            va_name_figure_loss_best_epoch,\n",
    "            va_title_best_epoch=va_title_best_epoch,\\\n",
    "            va_folder_figures=va_folder_figures,\n",
    "            va_name_figure_best_traind_model=\\\n",
    "            va_name_figure_best_traind_model,\n",
    "            va_name_figure_loss_best_trained_model=\\\n",
    "            va_name_figure_loss_best_trained_model,\n",
    "            va_name_figure_plots_test_set=\\\n",
    "            va_name_figure_plots_test_set,\n",
    "            va_name_file_plot_graph=va_name_file_plot_graph,\n",
    "            va_nb_takes_plot_inferences=\\\n",
    "            va_nb_takes_plot_inferences,\n",
    "            val_x_label=\"Best Model ID\",\n",
    "            val_y_label=\"Value Metric\",\n",
    "            val_title=\"Metrics Test Set Per Each Best Model\",\n",
    "            val_title_best_trained_model=\\\n",
    "            \"Train Set Metrics - Retrained Best Model\",\n",
    "            va_loc=\"best\",\\\n",
    "            val_title_inferences_plot=\"Inferences versus True Future Values\")\n",
    "\n",
    "        elif va_distinct_origins==1:\n",
    "            fct_analyze_results_2(\n",
    "            va_di_responses=va_li_or_di_responses,\n",
    "            va_folder_figures=va_folder_figures,\n",
    "            va_mae_test_set=va_mae_test_set,\n",
    "            va_rmse_test_set=va_rmse_test_set,\n",
    "            va_li_colors=va_li_colors,\n",
    "            va_li_markers=va_li_markers,\n",
    "            va_name_figure_metric_for_predicts=\\\n",
    "            va_name_figure_metric_for_predicts,   \n",
    "            va_name_figure_find_best_epoch=\\\n",
    "            va_name_figure_find_best_epoch,\n",
    "            va_name_figure_loss_best_epoch=\\\n",
    "            va_name_figure_loss_best_epoch,\n",
    "            va_title_best_epoch=va_title_best_epoch,\n",
    "            va_name_figure_best_traind_model=\\\n",
    "            va_name_figure_best_traind_model,\n",
    "            va_name_figure_loss_best_trained_model=\\\n",
    "            va_name_figure_loss_best_trained_model,\n",
    "            va_name_figure_plots_test_set=\\\n",
    "            va_name_figure_plots_test_set,\n",
    "            va_name_file_plot_graph=\\\n",
    "            va_name_file_plot_graph,\n",
    "            va_nb_takes_plot_inferences=\\\n",
    "            va_nb_takes_plot_inferences,\n",
    "            val_x_label=\"Best Model ID\",\n",
    "            val_y_label=\"Value Metric\",\n",
    "            val_title=\"Metrics Test Set Per Each Best Model\",\n",
    "            val_title_best_trained_model=\\\n",
    "            \"Train Set Metrics - Retrained Best Model\",\n",
    "            va_loc=\"best\",\\\n",
    "            val_title_inferences_plot=\"Inferences versus True Future Values\")\n",
    "\n",
    "        else:\n",
    "            print(\"PROBLEM IN FCT fct_analyze_results, va_distinct_origins: \",va_distinct_origins)\n",
    "            import sys\n",
    "            sys.exit()\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012830d-7584-4adb-96ba-7cb0be36486f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6882a0-a525-4350-843c-28ae2432a729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be98c5f-d5d1-443f-8245-15e248ff7882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9834e03-b525-4899-a0dd-29f2aa421b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142902cb-1382-41e1-b6bc-c14302985a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
