{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f734806e-ac42-4181-92f9-6a3e908793de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from cc_fi_models_1.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-30 20:52:08.662049: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import pickle\n",
    "\n",
    "#import it from the notebook\n",
    "import import_ipynb\n",
    "\n",
    "#import the file with the NN model\n",
    "#import cc_fi_models_1 \n",
    "from cc_fi_models_1 import *\n",
    "\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212d2be3-2a45-4539-8b4c-4e9fe244e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read DataFrame from pickle file\n",
    "df_2009_1= pd.read_pickle(\"my_df_2009_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff58f7-f303-42ee-986d-fb435d260dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933a80b8-1ac3-4a97-bfe1-3e346a19d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "\n",
    "#variables\n",
    "\n",
    "\n",
    "#if val_distinct_origins=0 we employ the initial dataframe\n",
    "#for studying all the origins together\n",
    "##if val_distinct_origins=1 we employ the initial dataframe\n",
    "#for studying each origin separately\n",
    "val_distinct_origins=0\n",
    "#val_distinct_origins=1\n",
    "\n",
    "#the min number of admissible observations for a dataframe to be considered\n",
    "#this variable is employed when studying each origin airport separtly\n",
    "#if  the dataframe created for a give origin has fewer entries than\n",
    "#the value of this variable then the dataframe is not treated\n",
    "val_admissible_min_nb_observatios=80\n",
    "\n",
    "\n",
    "#the name of the folder with  the figures (plots)\n",
    "val_folder_figures=\"./Figures_Model\"\n",
    "\n",
    "\n",
    "#the name of the dataframe to use\n",
    "#after treating the inputs (missing values etc.).\n",
    "val_dataframe=df_2009_1\n",
    "\n",
    "#value of the stride when creating \n",
    "#train, validation and test sets\n",
    "#using kears function \n",
    "#timeseries_dataset_from_array\n",
    "#it corresponds to the desired distance \n",
    "#betwenne consecutive sequences\n",
    "val_s_stride=1\n",
    "\n",
    "\n",
    "#the value of the batch when creating\n",
    "#train, validation and test sets\n",
    "val_batch_size=256\n",
    "\n",
    "\n",
    "#the number of  sequence lengths sets to use \n",
    "#for the learning\n",
    "#in the current model a sequence length for learning\n",
    "#is comprised of the  \n",
    "#mean value of observations per month\n",
    "#if val_nb_past_seq_lengths>1 we increase the number\n",
    "#of saples to use for learning\n",
    "val_nb_past_seq_lengths=1\n",
    "\n",
    "##the number of  sequence lengths sets to use \n",
    "#for inferences\n",
    "#in the current model a sequence length \n",
    "#for forecast is comprised of the  \n",
    "#mean value of observations per month\n",
    "#if val_nb_future_seq_lengths>1 we increase the number\n",
    "#of saples to use for inference\n",
    "val_nb_future_seq_lengths=1\n",
    "\n",
    "#variable indicating th desired \n",
    "#sequence length to use for learning\n",
    "#it values none as we wish to use \n",
    "#the mean number of observation per month\n",
    "val_s_length_train_model=None\n",
    "\n",
    "#variable indicating th desired \n",
    "#sequence length to use for inference\n",
    "#it values none as we wish to use \n",
    "#the mean number of observation per month\n",
    "val_s_length_target=None\n",
    "\n",
    "#the name of the column with the Origins \n",
    "#nams of the origin airorts\n",
    "#this  name is proper to the provided dataset\n",
    "#df_2009_1\n",
    "val_name_col_origin=\"ORIGIN\"\n",
    "\n",
    "\n",
    "#the name of the column with the Destinations \n",
    "#nams of the destination airorts\n",
    "#this  name is proper to the provided dataset\n",
    "#df_2009_1\n",
    "val_li_name_col_to_copy=['DEST']\n",
    "\n",
    "#the name of the column with the flying dates\n",
    "##this  name is proper to the provided dataset\n",
    "#df_2009_1\n",
    "val_name_column_date='FL_DATE'\n",
    "\n",
    "#the name of the column \n",
    "#to sort the dataframe df_2009_1\n",
    "#it is the departure time\n",
    "val_name_col_2_sort='DEP_TIME'\n",
    "\n",
    "#the name of the targer variable \n",
    "#(column of datframe df_2009_1)\n",
    "val_name_target_variable=\"ARR_DELAY\"\n",
    "\n",
    "\n",
    "#list with the names of variables (features)\n",
    "#to ignore when we vectorize  the dataframe\n",
    "#so as to have numerical variables \n",
    "val_li_cols_to_ignore=['FL_DATE','ORIGIN']\n",
    "\n",
    "#the proportion of the data to use for training\n",
    "val_proportion_train_set=0.5\n",
    "\n",
    "#the proportion of data to use for validation\n",
    "#the remaining 0.25 of the dataset will be\n",
    "#used for the test set\n",
    "val_proportion_val_set=0.25\n",
    "\n",
    "val_name_fig=\"fig_cor_vector_data_heatmap.png\"\n",
    "\n",
    "#variables indicating whether to shuffle (True)  \n",
    "#the output samples created using keras\n",
    "#timeseries_dataset_from_array\n",
    "#or to respect the chronological order (False)\n",
    "val_shuffle_tr_s=False\n",
    "val_shuffle_tr_t=False\n",
    "val_shuffle_v_s=False\n",
    "val_shuffle_v_t=False\n",
    "val_shuffle_t_s=False\n",
    "val_shuffle_t_t=False\n",
    "\n",
    "\n",
    "#**********variables for the keras_tuner model\n",
    "\n",
    "#the  min nunber of layers to select\n",
    "val_min_nb_lay_model=1\n",
    "#val_min_nb_lay_model=2\n",
    "\n",
    "#the  max nunber of layers to select\n",
    "#val_max_nb_lay_model=10\n",
    "#val_max_nb_lay_model=8\n",
    "val_max_nb_lay_model=2\n",
    "\n",
    "#the min number of units (nb features+1)\n",
    "val_min_nb_units_model=14\n",
    "\n",
    "#the  max nunber of units (of a single layer) to select\n",
    "#val_max_nb_units_model=20\n",
    "val_max_nb_units_model=17\n",
    "\n",
    "#the min admissible value for the dropout rate\n",
    "val_min_value_dropout_rate_model=0.1\n",
    "\n",
    "#the max admissible value for the dropout rate\n",
    "val_max_value_dropout_rate_model=0.5\n",
    "\n",
    "#the min admissible value for the \n",
    "#recurrent dropout rate (RNN models)\n",
    "val_min_value_recurrent_dropout_rate_model=0.1\n",
    "#val_min_value_recurrent_dropout_rate_model=None\n",
    "\n",
    "#the max admissible value for the \n",
    "#recurrent dropout rate (RNN models)\n",
    "val_max_value_recurrent_dropout_rate_model=0.5\n",
    "#val_max_value_recurrent_dropout_rate_model=None\n",
    "\n",
    "#the min value of the learning rate\n",
    "val_min_val_learning_rate_optimizer=1e-4\n",
    "\n",
    "#the max value of the learning rate\n",
    "val_max_val_learning_rate_optimizer=1e-2\n",
    "\n",
    "#**********variables for conv1D layer\n",
    "\n",
    "#the min admissible value for the filters\n",
    "#val_min_nb_filters_conv1d=None\n",
    "val_min_nb_filters_conv1d=6\n",
    "\n",
    "#the max admissible value for the filters\n",
    "#val_max_nb_filters_conv1d=None\n",
    "val_max_nb_filters_conv1d=18\n",
    "\n",
    "#the min admissible value for the kernel size\n",
    "#val_min_nb_kernel_size_conv1d=None\n",
    "val_min_nb_kernel_size_conv1d=20\n",
    "\n",
    "#the max admissible value for the kernel size\n",
    "#val_max_nb_kernel_size_conv1d=None\n",
    "val_max_nb_kernel_size_conv1d=50\n",
    "\n",
    "#the step when seecting a value for the kernel size\n",
    "#val_step_nb_kernel_size_conv1d=None\n",
    "val_step_nb_kernel_size_conv1d=10\n",
    "\n",
    "#the value for the pool size for the\n",
    "#maxpooling layer1\n",
    "#val_pool_size=2\n",
    "val_min_pool_size=2\n",
    "val_max_pool_size=8\n",
    "val_step_pool_size=None\n",
    "#**********\n",
    "\n",
    "#the step when selcting the number of layers\n",
    "#val_step_nb_layers_model=1\n",
    "val_step_nb_layers_model=None\n",
    "\n",
    "#the step when selecting the value \n",
    "#for the recurrent dropout rate\n",
    "val_step_recurrent_dropout_rate_model=None\n",
    "\n",
    "#the step when selecting the number of units \n",
    "#of a layer (Dense,RNN)\n",
    "#val_step_nb_units_model=1\n",
    "val_step_nb_units_model=None\n",
    "\n",
    "\n",
    "#the step when selecting the dropout rate\n",
    "#val_step_dropout_rate_model=0.01\n",
    "val_step_dropout_rate_model=None\n",
    "\n",
    "#the list of activation functions (Dense)\n",
    "#val_li_activ_fcts_model=['swish', 'elu', 'relu','selu']\n",
    "val_li_activ_fcts_model=['relu','elu','selu']\n",
    "\n",
    "#for RNN\n",
    "#val_li_activ_fcts_model=['tanh','relu','elu','selu']\n",
    "\n",
    "\n",
    "#the number of units of the last layer\n",
    "#val_nb_last_output_classes_model=1\n",
    "\n",
    "#the list of the optimizers for the tuner\n",
    "#an optimizer specifies \n",
    "#how the gradient of the loss will be used \n",
    "#to update parameters.\n",
    "#val_li_optimizers_model=[\"rmsprop\",\"adam\",\"adadelta\",\"adagrad\",\"adamax\"]\n",
    "val_li_optimizers_model=[\"rmsprop\",\"adam\",\"adagrad\"]\n",
    "\n",
    "\n",
    "#the list with the loss functions to consider\n",
    "#the quantity we are trying to minimize during the training\n",
    "\n",
    "#va_loss_fct_model=\"mse\"\n",
    "#val_loss_fct_model=[\"mse\",\"mean_absolute_percentage_error\",\\\n",
    "#                   \"mean_squared_logarithmic_error\",\"mae\"]\n",
    "val_loss_fct_model=[\"mse\"]\n",
    "\n",
    "#va_loss_fct_model=[\"mse\",tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "#name=\"mean_squared_logarithmic_error\")]\n",
    "\n",
    "\n",
    "#the list with the metrics to evaluate the model performance\n",
    "#va_metrics_model=\"mae\"\n",
    "#val_metrics_model=va_metrics_model=[\"mae\",\\\n",
    "#tensorflow.keras.losses.Huber(name=\"huber_loss\"),\"mean_squared_logarithmic_error\",\\\n",
    "#\"mean_absolute_percentage_error\",\\\n",
    "#                                   tensorflow.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "#tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "#name=\"mean_squared_logarithmic_error\"),\"mean_absolute_percentage_error\"]\n",
    "val_metrics_model=va_metrics_model=[\"mae\"]\n",
    "\n",
    "\n",
    "#the metric for the tuner to optimize\n",
    "val_objective_metric_for_tuner_to_optimize=\"val_mae\"\n",
    "#val_objective_metric_for_tuner_to_optimize=\"val_mean_squared_logarithmic_error\"\n",
    "#\"val_rmse\"\n",
    "\n",
    "#the max number of different model configurations\n",
    "#to try during the tuner search\n",
    "#val_max_trials=1\n",
    "val_max_trials=2\n",
    "#val_max_trials=4\n",
    "#val_max_trials=12\n",
    "#val_max_trials=20\n",
    "\n",
    "#varibale indicating the number of times to train the\n",
    "#same model.\n",
    "#in order to reduce the variance one can average the results\n",
    "#val_executions_per_trial=1\n",
    "val_executions_per_trial=2\n",
    "\n",
    "#path to a directory for storing the search results\n",
    "val_directory=\"flight_del_kt_test\"\n",
    "\n",
    "#variable indicating\n",
    "#Whether to overwrite data \n",
    "#in directory to start a new search\n",
    "val_overwrite=True\n",
    "\n",
    "\n",
    "#variable indicating \n",
    "#the number of epochs with no improvement \n",
    "#after which training will be stopped\n",
    "#callback during the tuner search\n",
    "val_patience_during_tuner_search=5\n",
    "\n",
    "#the number of epochs to train each\n",
    "#considered configuration of a model\n",
    "# since we have an Earlystoping callback\n",
    "#we should use a large number\n",
    "#as training will syop when we start overfitting\n",
    "#val_epochs_tuner_search=1\n",
    "val_epochs_tuner_search=2\n",
    "#val_epochs_tuner_search=24\n",
    "#val_epochs=30\n",
    "\n",
    "\n",
    "#the number of epochs when we \n",
    "#search the best  number of epochs\n",
    "#for the best found models\n",
    "val_epochs_best_trained_model_search=2\n",
    "#val_epochs_best_trained_model_search=20\n",
    "\n",
    "\n",
    "#variable related to the verbosity during \n",
    "#the tuner search\n",
    "val_verbose=2\n",
    "\n",
    "#the number of best models to consider after the tuner search\n",
    "val_top_best_models=2\n",
    "\n",
    "#the parameter to multil the epochs\n",
    "#when retrain the best model\n",
    "#as we have more data (train+validation)\n",
    "#we retrain for longer\n",
    "val_to_multiply_epoch_for_train_dur=1.5\n",
    "\n",
    "\n",
    "#the metric's desired direction (min or max)\n",
    "# in the objective when we define the tuner\n",
    "#here we indicate that the metric \n",
    "#will have to be minimized\n",
    "val_mode=\"min\"\n",
    "\n",
    "\n",
    "#the direction of the objective \n",
    "#in the callbacks when\n",
    "#we search the best epoch\n",
    "val_mode_callbacks=\"min\"\n",
    "\n",
    "#the metric to monitor in the callbacks(best_model.fit)  when\n",
    "#we search the best epoch (\n",
    "val_metric_to_monitor_best_epoch_callbacks=\"val_loss\"\n",
    "\n",
    "\n",
    "#the metric for tuner when we search the best hyperparameters\n",
    "#(metric defined in the \"objective\")\n",
    "val_metric_for_tuner_search_hp_callback=\"val_loss\"\n",
    "\n",
    "#the patience when we search the best number of epochs for the\n",
    "#best  models\n",
    "#number of epochs to tlerate without improvement\n",
    "val_patience_best_epoch_callbacks=10\n",
    "\n",
    "#list titles for plots related to the best epoch\n",
    "val_li_label_1_for_plot_best_epoch=\\\n",
    "[\"Training Loss\",\"Training MAE\",\"Training RMSE\",\\\n",
    "'root_mean_squared_error']\n",
    "\n",
    "#the  list of colors to use for the  plots\n",
    "val_li_colors_for_plot_best_epoch=[\"crimson\",\"c\",\"m\",\"darkorange\"]\n",
    "\n",
    "#the location for the plot legends\n",
    "val_loc=\"best\"\n",
    "\n",
    "\n",
    "#*******\n",
    "\n",
    "\n",
    "#the name of the figure cotnaining the inferences versus \n",
    "#true values\n",
    "val_name_figure_metric_for_predicts=\"fig_inferences\"\n",
    "\n",
    "#the MAE of the test set\n",
    "#we consider None as we  will not use it\n",
    "val_mae_test_set=None\n",
    "\n",
    "#the RMSE of the test set\n",
    "#we consider None as we  will not use it\n",
    "val_rmse_test_set=None\n",
    "\n",
    "#***variables for plots when searching the best epoch for the best models\n",
    "\n",
    "\n",
    "#the name of the figure with the metrics plots \n",
    "#when  we search best epoch\n",
    "val_name_figure_find_best_epoch=\\\n",
    "\"fig_plot_metrics_search_best_epoch_best_retrained_model\"\n",
    "\n",
    "#the name of the figure with the plots \n",
    "#when we searchthe best  epoch\n",
    "val_name_figure_loss_best_epoch=\"Loss_Model\"\n",
    "\n",
    "#the title of the plot  when we search best epoch\n",
    "val_title_best_epoch=\\\n",
    "\"Metrics - Search Best Epoch for Best Model,\"\n",
    "\n",
    "\n",
    "#the name of the file where we store the best model\n",
    "val_pkl_filename_best_model=\"best_model\"\n",
    "\n",
    "\n",
    "#the name of the file where we sotre the ebst retrained model\n",
    "val_pkl_filename_best_retrained_model =\"history_obj_best_retrained_model\"   \n",
    "\n",
    "#***variables for plots of the best retrained model\n",
    "#val_di_hist_retrained_best_model=di_hist_retrained_best_model\n",
    "\n",
    "val_li_colors=[\"deepskyblue\",'r',\"gold\",\"violet\",\\\n",
    "              \"crimson\",\"mediumspringgreen\",\\\n",
    "             \"darkred\",\"yellow\",\"peru\",\"hotpink\",\"c\",\"lawngreen\"]\n",
    "\n",
    "val_li_markers=[\"o\",\"s\",\"v\",\"<\",\">\",\"P\",\"*\",\"X\",\"d\",\"s\",\".\",\"D\"]\n",
    "                \n",
    "val_name_figure_best_traind_model=\"fig_plot_metrics_best_retrained_model\"\n",
    "\n",
    "val_name_figure_loss_best_trained_model=\"Loss_Model\"\n",
    "\n",
    "#variables for plots on the test set\n",
    "#val_di_results_model_eval_test_set=di_results_model_eval_test_set\n",
    "val_name_figure_plots_test_set=\"fig_plot_metrics_test_set_per_best_model\"\n",
    "\n",
    "\n",
    "val_name_file_plot_graph=\"fig_graph_best_model\"\n",
    "\n",
    "\n",
    "#the number of plots comparing inferences with true values\n",
    "val_nb_takes_plot_inferences=3\n",
    "\n",
    "#************\n",
    "#the dictionary with the hypermodels\n",
    "#this dictionary is defined in file cc_fi_models_1.ipynb\n",
    "val_di_hypermodels=di_hypermodels\n",
    "\n",
    "#the dictionary with the tuners\n",
    "#this dictionary is defined in file cc_fi_models_1.ipynb\n",
    "val_di_tuners=di_tuners\n",
    "\n",
    "#the key in the dictionary defined in the file \n",
    "#cc_fi_models_1.ipynb\n",
    "#with the hypermodel classes\n",
    "#indicating the desired  hypermodel to evaluate\n",
    "val_key_hypermodel_class=1\n",
    "#val_key_hypermodel_class=2\n",
    "#val_key_hypermodel_class=3\n",
    "#val_key_hypermodel_class=4\n",
    "#val_key_hypermodel_class=5\n",
    "#val_key_hypermodel_class=6\n",
    "\n",
    "#val_key_hypermodel_class=7\n",
    "#val_key_hypermodel_class=8\n",
    "#val_key_hypermodel_class=9\n",
    "#val_key_hypermodel_class=10\n",
    "#val_key_hypermodel_class=11\n",
    "#val_key_hypermodel_class=12\n",
    "#val_key_hypermodel_class=13\n",
    "#val_key_hypermodel_class=14\n",
    "#val_key_hypermodel_class=15\n",
    "#val_key_hypermodel_class=21\n",
    "#val_key_hypermodel_class=22\n",
    "#val_key_hypermodel_class=23\n",
    "\n",
    "\n",
    "#the key in the dictionary with the tuner (defines in  file cc_models)\n",
    "#precising the tuner to use\n",
    "#1: BayesianOptimisation Tuner \n",
    "#2:BayesianOptimisation Tuner  with batch  size tuning\n",
    "#3:Gridsearch Tuner\n",
    "val_key_tuner_class=1\n",
    "#val_key_tuner_class=2\n",
    "#val_key_tuner_class=3\n",
    "\n",
    "\n",
    "val_li_keys_tuners_optimizing_batch_size=fct_li_keys_tuners_tuning_batch_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e59839-cb98-40e8-9713-cc4c3abb3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the folder for the figures (plots)\n",
    "\n",
    "#folder for the all the figure plots\n",
    "os.makedirs(val_folder_figures,exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49f6d6-a0bc-473e-87c0-8dc231015f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231ae56-9d20-4b96-869e-6cd6055054b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c833e-a8a5-4658-ac51-eb0eea194d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cac6da-fea3-4205-820f-6a532647fa11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c8c24-dff0-45a3-944b-bca33c1ac857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87265fb-9189-4cdb-aaeb-d1ba2ef29e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d9d91-5741-48a0-a1cb-405692617722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d9335-d90e-4066-8d86-82b0cea3c67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ecfa3-2182-4fa0-8846-47fb71c91bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b452a68-be95-43e8-b87e-1a6898b21685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb70d89-b4a1-4383-999d-9b60f1fe0437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94602b8e-50f1-4478-a45b-ff00d3269376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2469d-f5d6-4d48-9777-cb084be3c52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
