{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6b06e9-f29a-4cb8-b2df-a9564e3333c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-30 20:44:55.447734: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "import keras_tuner as kt\n",
    "\n",
    "import math\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff3ad7-4352-47d4-b746-067b9bc158d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d772547-ed24-492e-a3a5-2943d12f83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#IN THE FOOLOWING CLASS WE DEFINE THE SEARCH SPACE\n",
    "#we tune in the following order:\n",
    "#the number of layers\n",
    "#the number of units\n",
    "#the optimizer\n",
    "#the activation function\n",
    "#dropout - dropout rate\n",
    "#the loss function\n",
    "#the learning rate\n",
    "#batch size (in fit method)\n",
    "#********\n",
    "#we create a keras.Model by subclassing HyperModel class and\n",
    "#using the hp argument to define the hyperparameters\n",
    "\n",
    "#HyperModel instance to the Tuner\n",
    "\n",
    "#No batch Normalization\n",
    "\n",
    "class Dense_model_1(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None,\n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        self._min_nb_lay=val_min_nb_lay_model\n",
    "        \n",
    "        self._max_nb_lay=val_max_nb_lay_model\n",
    "        \n",
    "        self._min_nb_units=val_min_nb_units_model\n",
    "        \n",
    "        self._max_nb_units=val_max_nb_units_model\n",
    "        \n",
    "        self._step_nb_layers=val_step_nb_layers_model\n",
    "        \n",
    "        self._step_nb_units=val_step_nb_units_model\n",
    "        \n",
    "        self._li_activ_fcts=val_li_activ_fcts_model\n",
    "        \n",
    "        self._nb_last_output_classes=val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "        \n",
    "        \n",
    "    \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a Fully connected model.\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs = keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "        \n",
    "        new_feat= layers.Flatten()(inputs)\n",
    "\n",
    "        nb_lay=hp.Int('num_layers',\n",
    "                              min_value=self._min_nb_lay, \n",
    "                              max_value=self._max_nb_lay,\n",
    "                              step=self._step_nb_layers)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(nb_lay):\n",
    "            \n",
    "            \n",
    "            new_feat=layers.Dense(\\\n",
    "                units=hp.Int(name=\"units_lay_\"+str(i),\\\n",
    "                             min_value=self._min_nb_units,\\\n",
    "                             max_value=self._max_nb_units,\\\n",
    "                             step=self._step_nb_units),\\\n",
    "                activation=\\\n",
    "                hp.Choice(name = 'activation_'+str(i),\\\n",
    "                          values = self._li_activ_fcts,\\\n",
    "                          ordered = False))(new_feat)\n",
    "            \n",
    "            \n",
    "        \n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        new_feat=layers.Dense(\\\n",
    "                         units=self._nb_last_output_classes,\n",
    "                         )(new_feat)\n",
    "        \n",
    "        #we create the keras model\n",
    "        model = tensorflow.keras.Model(inputs=inputs, outputs=new_feat)\n",
    "        \n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer= hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "        \n",
    "        loss_hp=hp.Choice(name = 'loss',\n",
    "        values = self._loss,ordered = False)\n",
    "                        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_hp,\n",
    "            #loss=self._loss,\n",
    "            metrics=self._metrics)\n",
    "        \n",
    "        \n",
    "        learning_rate =\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer, sampling=\"log\")\n",
    "        \n",
    "        \n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "\n",
    "        def fit(self, hp, model, *args, **kwargs):\n",
    "            return model.fit(\n",
    "                *args,\n",
    "               batch_size=hp.Choice(\"batch_size\", [32, 1024]),\n",
    "                **kwargs)\n",
    "        \n",
    "        return model\n",
    "            \n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721c429a-127f-41ff-aa35-78f6166edda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LayerNormalization\n",
    "class Dense_model_2(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None, \n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        self._min_nb_lay=val_min_nb_lay_model\n",
    "        \n",
    "        self._max_nb_lay=val_max_nb_lay_model\n",
    "        \n",
    "        self._min_nb_units=val_min_nb_units_model\n",
    "        \n",
    "        self._max_nb_units=val_max_nb_units_model\n",
    "        \n",
    "        \n",
    "        self._step_nb_layers=val_step_nb_layers_model\n",
    "        \n",
    "        self._step_nb_units=val_step_nb_units_model\n",
    "        \n",
    "    \n",
    "        self._li_activ_fcts_model=val_li_activ_fcts_model\n",
    "        \n",
    "        self._nb_last_output_classes=val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "        \n",
    "        \n",
    "    \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a Fully connected model.\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs = keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "        \n",
    "        new_feat= layers.Flatten()(inputs)\n",
    "\n",
    "        nb_lay=\\\n",
    "        hp.Int('num_layers',min_value=self._min_nb_lay,\\\n",
    "               max_value=self._max_nb_lay,step=self._step_nb_layers)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(nb_lay):\n",
    "\n",
    "            if hp.Boolean(\"layernormalization_\"+str(i)):\n",
    "                \n",
    "                \n",
    "\n",
    "                #since normalization will center the output to zero\n",
    "                #the  bias vector is not neded\n",
    "                #if relu is selected as activation function\n",
    "                #according to some bibliography \n",
    "                #doing normalization before the activation \n",
    "                #maximizes the utilization of the relu.\n",
    "                #but this ordering is not very critical\n",
    "                new_feat=layers.Dense(\\\n",
    "                    units=\\\n",
    "                    hp.Int(name=\"units_lay_\"+str(i),\\\n",
    "                           min_value=self._min_nb_units,\\\n",
    "                           max_value=self._max_nb_units,\\\n",
    "                           step=self._step_nb_units),\\\n",
    "                use_bias=False)(new_feat)\n",
    "\n",
    "                #print(\"drop_rate\",drop_rate)\n",
    "                new_feat=layers.LayerNormalization()(new_feat)\n",
    "\n",
    "                act_fct=\\\n",
    "                hp.Choice(name='activation_'+str(i),\\\n",
    "                          values=self._li_activ_fcts_model,\\\n",
    "                          ordered=False)\n",
    "\n",
    "                new_feat=layers.Activation(act_fct)(new_feat)\n",
    "                \n",
    "            else:\n",
    "\n",
    "                new_feat=layers.Dense(\\\n",
    "                    units=\\\n",
    "                    hp.Int(name=\"units_lay_\"+str(i),\\\n",
    "                           min_value=self._min_nb_units,\\\n",
    "                           max_value=self._max_nb_units,\\\n",
    "                           step=self._step_nb_units),\\\n",
    "                    activation=\\\n",
    "                    hp.Choice(name='activation_'+str(i),\\\n",
    "                              values=self._li_activ_fcts_model,\\\n",
    "                              ordered=False))(new_feat)\n",
    "\n",
    "\n",
    "            \n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        new_feat=layers.Dense(units=self._nb_last_output_classes)(new_feat)\n",
    "        #we create the keras model\n",
    "        model=tensorflow.keras.Model(inputs=inputs, outputs=new_feat)\n",
    "\n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer=\\\n",
    "        hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "\n",
    "        loss_hp=\\\n",
    "        hp.Choice(name = 'loss',values = self._loss,ordered = False)\n",
    "\n",
    "        model.compile(\\\n",
    "            optimizer=optimizer,loss=loss_hp,metrics=self._metrics)\n",
    "\n",
    "        learning_rate=\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer,\\\n",
    "                 sampling=\"log\")\n",
    "\n",
    "        \n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "        return model\n",
    "\n",
    "                                  \n",
    "            \n",
    "            \n",
    "            \n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf78f5a-fae9-49a7-b465-566fdbcdcead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b9f2d6-e9dc-435a-8ec0-50788f09b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Dropout\n",
    "class Dense_model_3(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None, \n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        self._min_nb_lay=val_min_nb_lay_model\n",
    "        \n",
    "        self._max_nb_lay=val_max_nb_lay_model\n",
    "        \n",
    "        self._min_nb_units=val_min_nb_units_model\n",
    "        \n",
    "        self._max_nb_units=val_max_nb_units_model\n",
    "        \n",
    "        self._min_value_dropout_rate=val_min_value_dropout_rate_model\n",
    "        \n",
    "        self._max_value_dropout_rate=val_max_value_dropout_rate_model \n",
    "\n",
    "        \n",
    "        self._step_nb_layers=val_step_nb_layers_model\n",
    "        \n",
    "        self._step_nb_units=val_step_nb_units_model\n",
    "        \n",
    "        self._step_dropout_rate=val_step_dropout_rate_model\n",
    "\n",
    "        \n",
    "        self._li_activ_fcts_model=val_li_activ_fcts_model\n",
    "        \n",
    "        self._nb_last_output_classes=val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "        \n",
    "        \n",
    "    \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a Fully connected model.\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs = keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "        \n",
    "        new_feat= layers.Flatten()(inputs)\n",
    "\n",
    "        #the drop out rate if a dropout is selected \n",
    "        drop_rate = hp.Float(\"dropout_rate\",\\\n",
    "        min_value=self._min_value_dropout_rate,\\\n",
    "        max_value=self._max_value_dropout_rate,\\\n",
    "        step=self._step_dropout_rate)\n",
    "\n",
    "        nb_lay=\\\n",
    "        hp.Int('num_layers',min_value=self._min_nb_lay,max_value=self._max_nb_lay,step=self._step_nb_layers)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(nb_lay):\n",
    "            \n",
    "            new_feat=layers.Dense(\\\n",
    "                    units=\\\n",
    "                    hp.Int(name=\"units_lay_\"+str(i),\\\n",
    "                           min_value=self._min_nb_units,\\\n",
    "                           max_value=self._max_nb_units,\\\n",
    "                           step=self._step_nb_units),\\\n",
    "                    activation=\\\n",
    "                    hp.Choice(name='activation_'+str(i),\\\n",
    "                              values=self._li_activ_fcts_model,\\\n",
    "                              ordered=False))(new_feat)\n",
    "\n",
    "            if hp.Boolean(\"dropout_\"+str(i)):\n",
    "\n",
    "                #print(\"drop_rate\",drop_rate)\n",
    "                new_feat = tensorflow.keras.layers.Dropout(rate=drop_rate)(new_feat)\n",
    "\n",
    "\n",
    "            \n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        new_feat=layers.Dense(units=self._nb_last_output_classes)(new_feat)\n",
    "        \n",
    "        #we create the keras model\n",
    "        model=tensorflow.keras.Model(inputs=inputs, outputs=new_feat)\n",
    "\n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer=\\\n",
    "        hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "\n",
    "        loss_hp=\\\n",
    "        hp.Choice(name = 'loss',values = self._loss,ordered = False)\n",
    "\n",
    "        model.compile(\\\n",
    "            optimizer=optimizer,loss=loss_hp,metrics=self._metrics)\n",
    "\n",
    "        learning_rate=\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer,\\\n",
    "                 sampling=\"log\")\n",
    "\n",
    "\n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "        return model\n",
    "\n",
    "                                  \n",
    "            \n",
    "            \n",
    "            \n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a0e7e8-cfef-4556-87fe-58f995f36690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add model 3_1 using layer normal+ drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff1deea-bfaf-482d-bb30-966acc6d6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple convolutional 1D model\n",
    "\n",
    "class SimpleConv1D_model_1(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None,\n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        \n",
    "        self._nb_last_output_classes=val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "         \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a simple convolutional 1D  model .\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs = keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "        \n",
    "\n",
    "        x = layers.Conv1D(filters=8, kernel_size=24, activation=\"relu\")(inputs)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.Conv1D(filters=8, kernel_size=12, activation=\"relu\")(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.Conv1D(filters=8, kernel_size=6, activation=\"relu\")(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        outputs = layers.Dense(self._nb_last_output_classes)(x)\n",
    "\n",
    "        model = keras.Model(inputs, outputs)\n",
    "        \n",
    "        #the. last layer will  have the appropriate nb of units\n",
    "        #since we have a regressionproblem the nb of units will be one\n",
    "       # x=layers.Dense(\\\n",
    "       # units=self._nb_last_output_classes)(x)\n",
    "        \n",
    "        ##we create the keras model\n",
    "        #model = tensorflow.keras.Model(inputs=inputs, outputs=x)\n",
    "        \n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer= hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "        \n",
    "        loss_hp=hp.Choice(name = 'loss',\n",
    "        values = self._loss,ordered = False)\n",
    "                        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_hp,\n",
    "            #loss=self._loss,\n",
    "            metrics=self._metrics)\n",
    "        \n",
    "        \n",
    "        learning_rate =\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer, sampling=\"log\")\n",
    "        \n",
    "        \n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "        \n",
    "        return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76739a29-27f2-4a86-b69c-2c621658d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LayerNormalization\n",
    "class SimpleConv1D_model_2(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None, \n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        self._min_nb_lay=val_min_nb_lay_model\n",
    "        \n",
    "        self._max_nb_lay=val_max_nb_lay_model\n",
    "        \n",
    "        self._min_nb_units=val_min_nb_units_model\n",
    "        \n",
    "        self._max_nb_units=val_max_nb_units_model\n",
    "        \n",
    "        \n",
    "        self._step_nb_layers=val_step_nb_layers_model\n",
    "        \n",
    "        self._step_nb_units=val_step_nb_units_model\n",
    "        \n",
    "\n",
    "        self._min_nb_filters_conv1d=\\\n",
    "        val_min_nb_filters_conv1d\n",
    "\n",
    "        self._max_nb_filters_conv1d=\\\n",
    "        val_max_nb_filters_conv1d\n",
    "\n",
    "        self._step_nb_filters_conv1d=\\\n",
    "        val_step_nb_filters_conv1d\n",
    "\n",
    "\n",
    "        self._min_nb_kernel_size_conv1d=\\\n",
    "        val_min_nb_kernel_size_conv1d\n",
    "\n",
    "        self._max_nb_kernel_size_conv1d=\\\n",
    "        val_max_nb_kernel_size_conv1d\n",
    "\n",
    "        self._val_step_nb_kernel_size_conv1d=\\\n",
    "        val_step_nb_kernel_size_conv1d\n",
    "\n",
    "        self._min_pool_size=val_min_pool_size\n",
    "\n",
    "        self._max_pool_size=val_max_pool_size\n",
    "\n",
    "        self._step_pool_size=val_step_pool_size\n",
    "        \n",
    "        self._nb_last_output_classes=\\\n",
    "        val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "        self._li_activ_fcts_model=val_li_activ_fcts_model\n",
    "       \n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "        \n",
    "        \n",
    "    \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a Fully connected model.\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs = keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "        \n",
    "    \n",
    "\n",
    "        x=inputs\n",
    "\n",
    "        nb_lay=\\\n",
    "        hp.Int('num_layers',min_value=self._min_nb_lay,\\\n",
    "               max_value=self._max_nb_lay,step=self._step_nb_layers)\n",
    "\n",
    "        \n",
    "\n",
    "        nb_filters=\\\n",
    "        hp.Int('num_filters',min_value=self._min_nb_filters_conv1d,\\\n",
    "               max_value=self._max_nb_filters_conv1d,\\\n",
    "               step=self._step_nb_filters_conv1d)\n",
    "\n",
    "\n",
    "        nb_kernel_size=\\\n",
    "        hp.Int('kernel_size'\\\n",
    "               ,min_value=self._min_pool_size,\\\n",
    "               max_value=self._max_pool_size,\\\n",
    "               step=self._val_step_nb_kernel_size_conv1d)\n",
    "\n",
    "\n",
    "        nb_f=nb_filters\n",
    "\n",
    "        nb_ks=nb_kernel_size\n",
    "\n",
    "        ps=\\\n",
    "        hp.Int('pool_size',\\\n",
    "               min_value=self._min_nb_kernel_size_conv1d,\\\n",
    "               max_value=self._max_nb_kernel_size_conv1d,\\\n",
    "               step=self._val_step_nb_kernel_size_conv1d)\n",
    "\n",
    "\n",
    "        act_fct=\\\n",
    "        hp.Choice(name='activation',\\\n",
    "                  values=self._li_activ_fcts_model,\\\n",
    "                  ordered=False)\n",
    "        \n",
    "        \n",
    "        for i in range(nb_lay):\n",
    "            \n",
    "            #normalization centers outputs to zero\n",
    "            #so the bias vector is not needed\n",
    "            #according to some bibliography \n",
    "            #doing normalization before the activation \n",
    "            #maximizes the utilization of the relu.\n",
    "            #but this ordering is not very critical\n",
    "            if hp.Boolean(\"layernormalization_\"+str(i)):\n",
    "                \n",
    "               \n",
    "                \n",
    "                x=layers.Conv1D(\\\n",
    "                filters=nb_f,\\\n",
    "                kernel_size=nb_ks, use_bias=False)(x)\n",
    "                \n",
    "                x=layers.LayerNormalization()(x)\n",
    "                \n",
    "                x=layers.Activation(act_fct)(x)\n",
    "            else:\n",
    "                x=layers.Conv1D(\\\n",
    "                filters=nb_f,\\\n",
    "                kernel_size=nb_ks, activation=act_fct)(x)\n",
    "                \n",
    "\n",
    "            x = layers.MaxPooling1D(ps)(x)\n",
    "\n",
    "            if math.ceil(nb_ks/2)>0:\n",
    "                nb_ks=math.ceil(nb_ks/2)\n",
    "            \n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        x=layers.Dense(units=self._nb_last_output_classes)(x)\n",
    "        \n",
    "        #we create the keras model\n",
    "        model=tensorflow.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer=\\\n",
    "        hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "\n",
    "        loss_hp=\\\n",
    "        hp.Choice(name = 'loss',values = self._loss,ordered = False)\n",
    "\n",
    "        model.compile(\\\n",
    "            optimizer=optimizer,loss=loss_hp,metrics=self._metrics)\n",
    "\n",
    "        learning_rate=\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer,\\\n",
    "                 sampling=\"log\")\n",
    "\n",
    "        \n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "        return model\n",
    "\n",
    "                                  \n",
    "            \n",
    "            \n",
    "            \n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1b9c30-ed00-4163-b164-ba58f2df807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#RNN\n",
    "#********\n",
    "#we create a keras.Model by subclassing HyperModel class and\n",
    "#using the hp argument to define the hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "#with BatchNormalization \n",
    "\n",
    "class RNN_model_1(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None,\n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        \n",
    "        self._nb_last_output_classes=val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "       \n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "        \n",
    "        \n",
    "    \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a simple RNN  model with LSTM layers.\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs = keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "\n",
    "        #the output will be (None, sequence length)\n",
    "        x=layers.LSTM(14,return_sequences=False,activation=\"tanh\")(inputs)\n",
    "        \n",
    "        \n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        x=layers.Dense(\\\n",
    "        units=self._nb_last_output_classes)(x)\n",
    "        \n",
    "        #we create the keras model\n",
    "        model = tensorflow.keras.Model(inputs=inputs, outputs=x)\n",
    "        \n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer= hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "        \n",
    "        loss_hp=hp.Choice(name = 'loss',\n",
    "        values = self._loss,ordered = False)\n",
    "                        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_hp,\n",
    "            #loss=self._loss,\n",
    "            metrics=self._metrics)\n",
    "        \n",
    "    \n",
    "        learning_rate =\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer, sampling=\"log\")\n",
    "        \n",
    "        \n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "        \n",
    "        return model\n",
    "            \n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1e3d5d-efe6-4bfd-a5a2-a0b5774fde1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#RNN\n",
    "#********\n",
    "#we create a keras.Model by subclassing HyperModel class and\n",
    "#using the hp argument to define the hyperparameters\n",
    "\n",
    "#HyperModel instance to the Tuner\n",
    "\n",
    "\n",
    "#with BatchNormalization \n",
    "\n",
    "class RNN_model_2(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None,\n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        \n",
    "        self._nb_last_output_classes=val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "        self._li_activ_fcts_model=val_li_activ_fcts_model\n",
    "       \n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "        \n",
    "        \n",
    "    \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a simple RNN  model with LSTM layers.\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs =\\\n",
    "        keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "\n",
    "        act_fct=\\\n",
    "        hp.Choice(name='activation',\\\n",
    "                  values = self._li_activ_fcts_model,\\\n",
    "                  ordered = False)\n",
    "        \n",
    "        x=layers.GRU(32, recurrent_dropout=0.25, return_sequences=True,\\\n",
    "                     activation=act_fct)(inputs)\n",
    "        x=layers.GRU(32, recurrent_dropout=0.25, return_sequences=False,\\\n",
    "                     activation=act_fct)(x)\n",
    "        x=layers.Dropout(0.5)(x)\n",
    "\n",
    "        \n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        x=layers.Dense(\\\n",
    "        units=self._nb_last_output_classes)(x)\n",
    "\n",
    "        \n",
    "        #we create the keras model\n",
    "        model = tensorflow.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "        \n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer= hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "\n",
    "       \n",
    "        loss_hp=hp.Choice(name = 'loss',\n",
    "        values = self._loss,ordered = False)\n",
    "\n",
    "        \n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_hp,\n",
    "            #loss=self._loss,\n",
    "            metrics=self._metrics)\n",
    "\n",
    "        \n",
    "        \n",
    "        learning_rate =\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer, sampling=\"log\")\n",
    "        \n",
    "        \n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "        \n",
    "        return model\n",
    "            \n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dda53f4-7546-4764-800e-e2db2f99d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#RNN\n",
    "#********\n",
    "#we create a keras.Model by subclassing HyperModel class and\n",
    "#using the hp argument to define the hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "#with BatchNormalization \n",
    "\n",
    "class RNN_model_3(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None,\n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        self._min_nb_lay=val_min_nb_lay_model\n",
    "        \n",
    "        self._max_nb_lay=val_max_nb_lay_model\n",
    "        \n",
    "        self._min_nb_units=val_min_nb_units_model\n",
    "        \n",
    "        self._max_nb_units=val_max_nb_units_model\n",
    "        \n",
    "        self._min_value_dropout_rate=val_min_value_dropout_rate_model\n",
    "        \n",
    "        self._max_value_dropout_rate=val_max_value_dropout_rate_model \n",
    "        \n",
    "        self._min_value_recurrent_dropout_rate_model=\\\n",
    "        val_min_value_recurrent_dropout_rate_model\n",
    "\n",
    "        self._max_value_recurrent_dropout_rate_model=\\\n",
    "        val_max_value_recurrent_dropout_rate_model\n",
    "        \n",
    "        self._step_nb_layers=val_step_nb_layers_model\n",
    "        \n",
    "        self._step_nb_units=val_step_nb_units_model\n",
    "        \n",
    "        self._step_dropout_rate=val_step_dropout_rate_model\n",
    "\n",
    "        self._step_recurrent_dropout_rate_model=\\\n",
    "        val_step_recurrent_dropout_rate_model\n",
    "        \n",
    "        self._nb_last_output_classes=val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "        self._li_activ_fcts_model=val_li_activ_fcts_model\n",
    "       \n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "        \n",
    "        \n",
    "    \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a simple RNN  model with LSTM layers.\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs = keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "        \n",
    "        \n",
    "        x=inputs\n",
    "      \n",
    "\n",
    "        a=hp.Int('num_layers',\\\n",
    "        min_value=self._min_nb_lay,\\\n",
    "        max_value=self._max_nb_lay,\\\n",
    "        step=self._step_nb_layers)\n",
    "        \n",
    "        #if a>1:\n",
    "        #val_return_sequences=True\n",
    "        #else:\n",
    "        val_return_sequences=False\n",
    "\n",
    "        \n",
    "        drop_rate = hp.Float(\"dropout_rate\",\n",
    "                             min_value=self._min_value_dropout_rate,\n",
    "                             max_value=self._max_value_dropout_rate,\n",
    "                             step=self._step_dropout_rate) \n",
    "\n",
    "        va_recurrent_dropout=\\\n",
    "        hp.Float('rec_droput_lay',\\\n",
    "        min_value=self._min_value_recurrent_dropout_rate_model,\\\n",
    "        max_value=self._max_value_recurrent_dropout_rate_model,\\\n",
    "        step=self._step_recurrent_dropout_rate_model)\n",
    "\n",
    "        #the activation function is selected\n",
    "        act_fct=\\\n",
    "        hp.Choice(name='activation',\\\n",
    "                    values = self._li_activ_fcts_model,\\\n",
    "                    ordered = False)\n",
    "        \n",
    "        for i in range(a):\n",
    "\n",
    "            #we ask the intermediate layers to return\n",
    "            #the full sequence of outputs\n",
    "            if i<int(a-1):\n",
    "                val_return_sequences=True\n",
    "            else:\n",
    "                val_return_sequences=False\n",
    "            \n",
    "            \n",
    "            #Each of the hyperparameters is uniquely identified \n",
    "            #by its name (the first argument). \n",
    "            #To tune the number of units in different Dense layers \n",
    "            #separately as different hyperparameters, \n",
    "            #we give them different names as f\"units_{i}\".\n",
    "            \n",
    "    \n",
    "\n",
    "            v_recurrent_dropout=va_recurrent_dropout\n",
    "\n",
    "            #the normalization wil center outputs to zero\n",
    "            #so bias vector is not needed\n",
    "            #if Layernormalization                     \n",
    "            if hp.Boolean(\"layernorm_\"+str(i)):\n",
    "\n",
    "                x=layers.LSTM(\\\n",
    "                         units=hp.Int(name=\"units_lay_\"+str(i), \n",
    "                         min_value=self._min_nb_units, \n",
    "                         max_value=self._max_nb_units, \n",
    "                         step=self._step_nb_units,\n",
    "                         ),return_sequences=val_return_sequences,\\\n",
    "                         recurrent_dropout=v_recurrent_dropout,\\\n",
    "                         use_bias=False)(x)\n",
    "\n",
    "                x=layers.LayerNormalization()(x)\n",
    "\n",
    "                x=layers.Activation(act_fct)(x)\n",
    "            else:\n",
    "                x=layers.LSTM(\\\n",
    "                         units=hp.Int(name=\"units_lay_\"+str(i), \n",
    "                         min_value=self._min_nb_units, \n",
    "                         max_value=self._max_nb_units, \n",
    "                         step=self._step_nb_units,\n",
    "                         ),return_sequences=val_return_sequences,\\\n",
    "                         recurrent_dropout=v_recurrent_dropout,\\\n",
    "                         activation=act_fct)(x)\n",
    "\n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "        #to regularize the dense layer we examine for a dropout layer after the\n",
    "        #LSTM\n",
    "        \n",
    "        #if hp.Boolean(\"dropout_\"+str(i)):\n",
    "            \n",
    "                \n",
    "            #print(\"drop_rate\",drop_rate)\n",
    "        x = tensorflow.keras.layers.Dropout(rate=drop_rate)(x)\n",
    "        \n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        x=layers.Dense(\\\n",
    "        units=self._nb_last_output_classes)(x)\n",
    "        \n",
    "        #we create the keras model\n",
    "        model = tensorflow.keras.Model(inputs=inputs, outputs=x)\n",
    "        \n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer= hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "        \n",
    "        loss_hp=hp.Choice(name = 'loss',\n",
    "        values = self._loss,ordered = False)\n",
    "                        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_hp,\n",
    "            #loss=self._loss,\n",
    "            metrics=self._metrics)\n",
    "        \n",
    "        \n",
    "        learning_rate =\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer, sampling=\"log\")\n",
    "        \n",
    "        \n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "        \n",
    "        return model\n",
    "            \n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af01bb-e531-40d0-989a-107c3eab140e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ef56b3-849d-49ba-93e8-f46443c02409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#un modele "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "619a4e6c-cc92-4d28-9841-ed95fb71be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#RNN\n",
    "#********\n",
    "#we create a keras.Model by subclassing HyperModel class and\n",
    "#using the hp argument to define the hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "#with BatchNormalization \n",
    "\n",
    "class RNN_model_4(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None,\n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        self._min_nb_lay=val_min_nb_lay_model\n",
    "        \n",
    "        self._max_nb_lay=val_max_nb_lay_model\n",
    "        \n",
    "        self._min_nb_units=val_min_nb_units_model\n",
    "        \n",
    "        self._max_nb_units=val_max_nb_units_model\n",
    "        \n",
    "        self._min_value_dropout_rate=val_min_value_dropout_rate_model\n",
    "        \n",
    "        self._max_value_dropout_rate=val_max_value_dropout_rate_model \n",
    "        \n",
    "        self._min_value_recurrent_dropout_rate_model=\\\n",
    "        val_min_value_recurrent_dropout_rate_model\n",
    "\n",
    "        self._max_value_recurrent_dropout_rate_model=\\\n",
    "        val_max_value_recurrent_dropout_rate_model\n",
    "        \n",
    "        self._step_nb_layers=val_step_nb_layers_model\n",
    "        \n",
    "        self._step_nb_units=val_step_nb_units_model\n",
    "        \n",
    "        self._step_dropout_rate=val_step_dropout_rate_model\n",
    "\n",
    "        self._step_recurrent_dropout_rate_model=\\\n",
    "        val_step_recurrent_dropout_rate_model\n",
    "        \n",
    "        self._nb_last_output_classes=val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "        self._li_activ_fcts_model=val_li_activ_fcts_model\n",
    "       \n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "        \n",
    "        \n",
    "    \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a simple RNN  model with LSTM layers.\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs = keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "        \n",
    "        \n",
    "        x=inputs\n",
    "        \n",
    "        a=hp.Int('num_layers',\\\n",
    "        min_value=self._min_nb_lay,\\\n",
    "        max_value=self._max_nb_lay,\\\n",
    "        step=self._step_nb_layers)\n",
    "        \n",
    "        #if a>1:\n",
    "        #val_return_sequences=True\n",
    "        #else:\n",
    "        val_return_sequences=False\n",
    "\n",
    "        \n",
    "        drop_rate = hp.Float(\"dropout_rate\",\n",
    "                             min_value=self._min_value_dropout_rate,\n",
    "                             max_value=self._max_value_dropout_rate,\n",
    "                             step=self._step_dropout_rate) \n",
    "\n",
    "        va_recurrent_dropout=\\\n",
    "        hp.Float('rec_droput_lay',\\\n",
    "        min_value=self._min_value_recurrent_dropout_rate_model,\\\n",
    "        max_value=self._max_value_recurrent_dropout_rate_model,\\\n",
    "        step=self._step_recurrent_dropout_rate_model)\n",
    "\n",
    "        #the activation function is selected\n",
    "        act_fct=\\\n",
    "        hp.Choice(name='activation',\\\n",
    "                  values = self._li_activ_fcts_model,\\\n",
    "                  ordered = False)\n",
    "        \n",
    "        for i in range(a):\n",
    "            \n",
    "            if i<int(a-1):\n",
    "                val_return_sequences=True\n",
    "            else:\n",
    "                val_return_sequences=False\n",
    "            \n",
    "            \n",
    "            #Each of the hyperparameters is uniquely identified \n",
    "            #by its name (the first argument). \n",
    "            #To tune the number of units in different Dense layers \n",
    "            #separately as different hyperparameters, \n",
    "            #we give them different names as f\"units_{i}\".\n",
    "            \n",
    "            \n",
    "            #if hp.Boolean(\"recurrent_dropout_\"+str(i)):\n",
    "                \n",
    "            #    v_recurrent_dropout=va_recurrent_dropout\n",
    "            #else:\n",
    "            #    v_recurrent_dropout=0\n",
    "                                  \n",
    "            #new_feat=layers.Activation(hp.Choice(name = 'activation_'+str(i),\n",
    "            #                   values = self._li_activ_fcts,\n",
    "            #                   ordered = False))(new_feat)\n",
    "                \n",
    "            #the activation function is selected\n",
    "            #act_fct=\\\n",
    "            #hp.Choice(name='activation_'+str(i),\\\n",
    "            #          values = self._li_activ_fcts_model,\\\n",
    "            #          ordered = False)\n",
    "\n",
    "            v_recurrent_dropout=va_recurrent_dropout\n",
    "\n",
    "            x=layers.Bidirectional(layers.LSTM(\\\n",
    "                         units=hp.Int(name=\"units_lay_\"+str(i), \n",
    "                         min_value=self._min_nb_units, \n",
    "                         max_value=self._max_nb_units, \n",
    "                         step=self._step_nb_units,\n",
    "                         ),return_sequences=val_return_sequences,\\\n",
    "                         recurrent_dropout=v_recurrent_dropout,\\\n",
    "                         activation=act_fct))(x)\n",
    "            \n",
    "            #if Layernormalization                     \n",
    "            #if hp.Boolean(\"layernorm_\"+str(i)):\n",
    "\n",
    "            #    x=layers.LayerNormalization()(x)\n",
    "\n",
    "                \n",
    "        \n",
    "\n",
    "       \n",
    "        #to regularize the dense layer we examine for a dropout layer after the\n",
    "        #LSTM\n",
    "        \n",
    "        #if hp.Boolean(\"dropout\"):\n",
    "            \n",
    "                \n",
    "            #print(\"drop_rate\",drop_rate)\n",
    "        x = tensorflow.keras.layers.Dropout(rate=drop_rate)(x)\n",
    "        \n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        x=layers.Dense(\\\n",
    "        units=self._nb_last_output_classes)(x)\n",
    "        \n",
    "        #we create the keras model\n",
    "        model = tensorflow.keras.Model(inputs=inputs, outputs=x)\n",
    "        \n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer= hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "        \n",
    "        loss_hp=hp.Choice(name = 'loss',\n",
    "        values = self._loss,ordered = False)\n",
    "                        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_hp,\n",
    "            #loss=self._loss,\n",
    "            metrics=self._metrics)\n",
    "        \n",
    "        \n",
    "        learning_rate =\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer, sampling=\"log\")\n",
    "        \n",
    "        \n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "        \n",
    "        return model\n",
    "            \n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb8e7d1-318b-4064-9ef8-66060ed0ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#RNN\n",
    "#********\n",
    "#we create a keras.Model by subclassing HyperModel class and\n",
    "#using the hp argument to define the hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "#with BatchNormalization \n",
    "\n",
    "class RNN_model_5(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self,\n",
    "    val_s_length_train_model,\n",
    "    val_nb_initial_input_features_model,\n",
    "    val_min_nb_lay_model=None,\n",
    "    val_max_nb_lay_model=None,\n",
    "    val_min_nb_units_model=None,\n",
    "    val_max_nb_units_model=None,\n",
    "    val_min_value_dropout_rate_model=None,\n",
    "    val_max_value_dropout_rate_model=None,\n",
    "    val_min_value_recurrent_dropout_rate_model=None,\n",
    "    val_max_value_recurrent_dropout_rate_model=None,\n",
    "    val_min_nb_filters_conv1d=None,\n",
    "    val_max_nb_filters_conv1d=None,\n",
    "    val_min_nb_kernel_size_conv1d=None,\n",
    "    val_max_nb_kernel_size_conv1d=None,\n",
    "    val_step_nb_layers_model=None,\n",
    "    val_step_nb_units_model=None,\n",
    "    val_step_dropout_rate_model=None,\n",
    "    val_step_recurrent_dropout_rate_model=None,\n",
    "    val_step_nb_filters_conv1d=None,\n",
    "    val_step_nb_kernel_size_conv1d=None,\n",
    "    val_min_pool_size=None,\n",
    "    val_max_pool_size=None,\n",
    "    val_step_pool_size=None,\n",
    "    val_li_activ_fcts_model=None,\n",
    "    val_nb_last_output_classes_model=None,\n",
    "    val_li_optimizers_model=None,\n",
    "    val_min_val_learning_rate_optimizer=None,\n",
    "    val_max_val_learning_rate_optimizer=None,\n",
    "    var_loss_fct_model=None,\n",
    "    #tensorflow.keras.metrics.MeanSquaredLogarithmicError(\n",
    "    #name=\"mean_squared_logarithmic_error\")],\n",
    "    var_metrics_model=None\n",
    "    ):\n",
    "        \n",
    "        self._s_length_train=val_s_length_train_model\n",
    "        \n",
    "        self._nb_initial_input_features=val_nb_initial_input_features_model\n",
    "        \n",
    "        self._min_nb_lay=val_min_nb_lay_model\n",
    "        \n",
    "        self._max_nb_lay=val_max_nb_lay_model\n",
    "        \n",
    "        self._min_nb_units=val_min_nb_units_model\n",
    "        \n",
    "        self._max_nb_units=val_max_nb_units_model\n",
    "        \n",
    "        self._min_value_dropout_rate=val_min_value_dropout_rate_model\n",
    "        \n",
    "        self._max_value_dropout_rate=val_max_value_dropout_rate_model \n",
    "        \n",
    "        self._min_value_recurrent_dropout_rate_model=\\\n",
    "        val_min_value_recurrent_dropout_rate_model\n",
    "\n",
    "        self._max_value_recurrent_dropout_rate_model=\\\n",
    "        val_max_value_recurrent_dropout_rate_model\n",
    "        \n",
    "        self._step_nb_layers=val_step_nb_layers_model\n",
    "        \n",
    "        self._step_nb_units=val_step_nb_units_model\n",
    "        \n",
    "        self._step_dropout_rate=val_step_dropout_rate_model\n",
    "\n",
    "        self._step_recurrent_dropout_rate_model=\\\n",
    "        val_step_recurrent_dropout_rate_model\n",
    "        \n",
    "        self._nb_last_output_classes=val_nb_last_output_classes_model\n",
    "                       \n",
    "        self._li_optimizers=val_li_optimizers_model\n",
    "\n",
    "        self._min_val_learning_rate_optimizer=\\\n",
    "        val_min_val_learning_rate_optimizer\n",
    "\n",
    "        self._max_val_learning_rate_optimizer=\\\n",
    "        val_max_val_learning_rate_optimizer\n",
    "        \n",
    "        self._loss=var_loss_fct_model\n",
    "        \n",
    "        self._metrics=var_metrics_model\n",
    "\n",
    "        self._li_activ_fcts_model=val_li_activ_fcts_model\n",
    "       \n",
    "\n",
    "        #hp = kt.HyperParameters() \n",
    "        \n",
    "        \n",
    "    \n",
    "    #method returning a compiled model \n",
    "    def build(self,hp):\n",
    "        \n",
    "        \"\"\"\n",
    "        Builds a simple RNN  model with LSTM layers.\n",
    "    \n",
    "        Args:\n",
    "        hp: instance  of Hyperparameter class of the Keras Tuner package, \n",
    "        This object defines the search space for the hyperparameter values\n",
    "        for a particular trial.\n",
    "        \n",
    "        Returns:\n",
    "        a Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        #we define the input layer\n",
    "        inputs = keras.Input(shape=(self._s_length_train,self._nb_initial_input_features))\n",
    "        \n",
    "        \n",
    "        x=inputs\n",
    "        \n",
    "\n",
    "        a=hp.Int('num_layers',\\\n",
    "        min_value=self._min_nb_lay,\\\n",
    "        max_value=self._max_nb_lay,\\\n",
    "        step=self._step_nb_layers)\n",
    "        \n",
    "        #if a>1:\n",
    "        #val_return_sequences=True\n",
    "        #else:\n",
    "        val_return_sequences=False\n",
    "\n",
    "        \n",
    "        drop_rate = hp.Float(\"dropout_rate\",\n",
    "                             min_value=self._min_value_dropout_rate,\n",
    "                             max_value=self._max_value_dropout_rate,\n",
    "                             step=self._step_dropout_rate) \n",
    "\n",
    "        va_recurrent_dropout=\\\n",
    "        hp.Float('rec_droput_lay',\\\n",
    "        min_value=self._min_value_recurrent_dropout_rate_model,\\\n",
    "        max_value=self._max_value_recurrent_dropout_rate_model,\\\n",
    "        step=self._step_recurrent_dropout_rate_model)\n",
    "\n",
    "\n",
    "        #the activation function is selected\n",
    "        act_fct=\\\n",
    "        hp.Choice(name='activation',\\\n",
    "                  values = self._li_activ_fcts_model,\\\n",
    "                  ordered = False)\n",
    "        \n",
    "        for i in range(a):\n",
    "            \n",
    "            if i<int(a-1):\n",
    "                val_return_sequences=True\n",
    "            else:\n",
    "                val_return_sequences=False\n",
    "            \n",
    "            \n",
    "            #Each of the hyperparameters is uniquely identified \n",
    "            #by its name (the first argument). \n",
    "            #To tune the number of units in different Dense layers \n",
    "            #separately as different hyperparameters, \n",
    "            #we give them different names as f\"units_{i}\".\n",
    "            \n",
    "            \n",
    "            v_recurrent_dropout=va_recurrent_dropout\n",
    "\n",
    "            \n",
    "            #if Layernormalization \n",
    "            #since normalization will center the output at zero\n",
    "            #no need of bias vector\n",
    "            if hp.Boolean(\"layernorm_\"+str(i)):\n",
    "\n",
    "                x=layers.Bidirectional(layers.GRU(\\\n",
    "                    units=hp.Int(name=\"units_lay_\"+str(i),\\\n",
    "                                 min_value=self._min_nb_units,\\\n",
    "                                 max_value=self._max_nb_units,\\\n",
    "                                 step=self._step_nb_units),\\\n",
    "                    return_sequences=val_return_sequences,\\\n",
    "                    recurrent_dropout=v_recurrent_dropout,\\\n",
    "                    use_bias=False))(x)\n",
    "                \n",
    "                x=layers.LayerNormalization()(x)\n",
    "\n",
    "                x=layers.Activation(act_fct)(x)\n",
    "            else:\n",
    "\n",
    "                x=layers.Bidirectional(layers.GRU(\\\n",
    "                    units=hp.Int(name=\"units_lay_\"+str(i),\\\n",
    "                                 min_value=self._min_nb_units,\\\n",
    "                                 max_value=self._max_nb_units,\\\n",
    "                                 step=self._step_nb_units),\\\n",
    "                    return_sequences=val_return_sequences,\\\n",
    "                    recurrent_dropout=v_recurrent_dropout,\\\n",
    "                    activation=act_fct))(x)\n",
    "\n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "        #to regularize the dense layer we examine for a dropout layer after the\n",
    "        #LSTM\n",
    "        \n",
    "        #if hp.Boolean(\"dropout\"):\n",
    "            \n",
    "                \n",
    "            #print(\"drop_rate\",drop_rate)\n",
    "        x = tensorflow.keras.layers.Dropout(rate=drop_rate)(x)\n",
    "        \n",
    "        #since we have a regression problem \n",
    "        #the nb of units of the last layer will be\n",
    "        #the number of timesteps to predict\n",
    "        #no activation of thelast layer \n",
    "        #so as to not constrain the output.\n",
    "        #If the last layer is linear the model wil learn\n",
    "        #to predict values of any range\n",
    "        x=layers.Dense(\\\n",
    "        units=self._nb_last_output_classes)(x)\n",
    "        \n",
    "        #we create the keras model\n",
    "        model = tensorflow.keras.Model(inputs=inputs, outputs=x)\n",
    "        \n",
    "        #in order to compile the model we need to define optimizers\n",
    "        optimizer= hp.Choice(name=\"optimizer\", values=self._li_optimizers)\n",
    "        \n",
    "        loss_hp=hp.Choice(name = 'loss',\n",
    "        values = self._loss,ordered = False)\n",
    "                        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_hp,\n",
    "            #loss=self._loss,\n",
    "            metrics=self._metrics)\n",
    "        \n",
    "        \n",
    "        learning_rate =\\\n",
    "        hp.Float(\"lr\", min_value=self._min_val_learning_rate_optimizer,\\\n",
    "                 max_value=self._max_val_learning_rate_optimizer, sampling=\"log\")\n",
    "        \n",
    "        \n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "        \n",
    "        return model\n",
    "            \n",
    "                         \n",
    "                         \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070723a-1c2e-429a-bb7d-66fd37f58753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c935d1-afac-4627-9433-4c7336a7fb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0e020-d220-49b2-9075-6169b302f5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d290f-f7fe-471a-b4d6-272b9279666d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfd334-91f6-4ea6-9fb8-c170304a7549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae944a5-bea7-491f-94c3-2ec4aa9599a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d616767-34c9-4199-9e8e-ee6dd6f3772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#di_models=dict,  AFAIRE BIEN\n",
    "#key = number, value=name class creating the hypermodel\n",
    "\n",
    "\n",
    "di_hypermodels={1:Dense_model_1,\\\n",
    "               2:Dense_model_2,\\\n",
    "               3:Dense_model_3,\\\n",
    "               4:SimpleConv1D_model_1,\\\n",
    "               5:SimpleConv1D_model_2,\\\n",
    "               6:RNN_model_1,\\\n",
    "               7:RNN_model_2,\\\n",
    "               8:RNN_model_3,\\\n",
    "               9:RNN_model_4,\\\n",
    "               10:RNN_model_5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf1707fc-a11a-424b-8306-39fdcbe9f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BayesianOptimizationTuner_j(kt.tuners.BayesianOptimization):\n",
    "    \n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        \n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', \\\n",
    "                                                        32,2048,32)\n",
    "        \n",
    "        return super(BayesianOptimizationTuner_j, self).run_trial(trial, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfe13ebc-6352-4530-9809-d83fad2a7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "di_tuners={1:kt.tuners.BayesianOptimization,2:BayesianOptimizationTuner_j,\n",
    "          3:kt.GridSearch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d79311a9-6959-493e-916f-adcc100fa224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list with the keys of the tuners which optimize the batch size\n",
    "#this list  must be updated whenever a new tuner tuning the batch size is considered\n",
    "val_li_keys_tuners_optimizing_batch_size=[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0555c8ce-6ae0-43b6-a74e-dfee9d5a0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct which returns t the list with the tuners tuning\n",
    "# the batch size\n",
    "def fct_li_keys_tuners_tuning_batch_size():\n",
    "    return val_li_keys_tuners_optimizing_batch_size\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714414b0-173e-4841-954f-28bdb6879a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42bef5-920e-48ea-915a-0d4006d705ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e08bc-b2f4-4f8c-9572-522a2eb07784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
